{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNkYK6RiqOK4R3R+dOeVe+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f67a139d927f4f8bb4a8c05998928ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c9af84ac1bb443da16068e90254dc4b",
              "IPY_MODEL_388472b0ae104320924c45cc999496cf",
              "IPY_MODEL_5376a37b9b934e869eced9466a97d100"
            ],
            "layout": "IPY_MODEL_10a4d2a201474fe6a87af8afdd867e03"
          }
        },
        "4c9af84ac1bb443da16068e90254dc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251e9559272c4a79b90bd440092b4493",
            "placeholder": "​",
            "style": "IPY_MODEL_44dfc7b6e8f541e2beb6bfbf07516de9",
            "value": "Computing transition probabilities: 100%"
          }
        },
        "388472b0ae104320924c45cc999496cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_134a2286c92a4c9c8ec08dbcd09a45ce",
            "max": 6651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cca2cfb40614b9fb296efc72376420d",
            "value": 6651
          }
        },
        "5376a37b9b934e869eced9466a97d100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4411ae07146b4acc8063f203e2614e4a",
            "placeholder": "​",
            "style": "IPY_MODEL_5db41fe0c94e4583970d7a5d76e5019d",
            "value": " 6651/6651 [04:47&lt;00:00, 19.60it/s]"
          }
        },
        "10a4d2a201474fe6a87af8afdd867e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251e9559272c4a79b90bd440092b4493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dfc7b6e8f541e2beb6bfbf07516de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "134a2286c92a4c9c8ec08dbcd09a45ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cca2cfb40614b9fb296efc72376420d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4411ae07146b4acc8063f203e2614e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5db41fe0c94e4583970d7a5d76e5019d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JITHIN-ANTONY-JOSEPH/ERP_11358080/blob/main/9_Experiment6_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input : Recipe Instructions\n",
        "### Model : Word2Vec(Text) + Node2Vec(Graph) + Neural Networks(Model) Version 1 -> 300 epochs"
      ],
      "metadata": {
        "id": "Md5Fhs4ZbyWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to connect to Google Drive"
      ],
      "metadata": {
        "id": "AUWIPnGyS5j9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbADcz6iu1mq",
        "outputId": "ff716cce-7597-418d-8a15-0713c3bebb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing required libraries"
      ],
      "metadata": {
        "id": "6J83wSzeTB97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIW_OsaCvCju",
        "outputId": "3d894e8e-bc25-4ea4-abc7-72525f3fac62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.66.5)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.16.0)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: node2vec\n",
            "Successfully installed node2vec-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries , loading datasets and pre-processing"
      ],
      "metadata": {
        "id": "9T-8FIX9S83g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q6FOAP2Ru5cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flavorgraph_df = pd.read_csv('/content/drive/My Drive/ERP/Dataset/nodes_191120.csv')  # Adjust the path as needed , this is the path to my personal Google Drive"
      ],
      "metadata": {
        "id": "5dyUMt6DvSv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import jellyfish\n",
        "from gensim.models import Word2Vec\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Load the main dataset\n",
        "with open('/content/drive/My Drive/ERP/modified_Processed_Layer1.json', 'r') as file: # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "    recipe1m_data = [json.loads(line) for line in file]\n",
        "\n",
        "recipe1m_df = pd.DataFrame(recipe1m_data)\n",
        "\n",
        "# Load the substitution pairs\n",
        "substitution_pairs_df = pd.read_csv('/content/drive/My Drive/ERP/Recipe1MSubs_full.csv') # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Merge the datasets based on recipe_id (substitution_pairs_df) and id (recipe1m_df)\n",
        "merged_df = pd.merge(recipe1m_df, substitution_pairs_df, left_on='id', right_on='recipe_id')\n",
        "\n",
        "# Example ingredient list for NER-like extraction (replace with your own comprehensive list or use NER model)\n",
        "ingredient_list = set(flavorgraph_df[flavorgraph_df['node_type'] == 'ingredient']['name'].dropna().unique())\n",
        "\n",
        "# Function to extract ingredients from instructions\n",
        "def extract_ingredients_from_instructions(instructions, ingredient_list):\n",
        "    extracted_ingredients = []\n",
        "    for instruction in instructions:\n",
        "        words = instruction.split()\n",
        "        for word in words:\n",
        "            if word in ingredient_list:\n",
        "                extracted_ingredients.append(word)\n",
        "    return extracted_ingredients\n",
        "\n",
        "# Apply the extraction function\n",
        "recipe1m_df['extracted_ingredients'] = recipe1m_df['processed_instructions'].apply(\n",
        "    lambda instructions: extract_ingredients_from_instructions(instructions, ingredient_list) if isinstance(instructions, list) else []\n",
        ")\n",
        "\n",
        "# Prepare sentences for training\n",
        "sentences = recipe1m_df['extracted_ingredients'].tolist()\n",
        "\n",
        "# Add substitution contexts to sentences\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ingredient1 = row['ingredient1']\n",
        "    ingredient2 = row['ingredient2']\n",
        "    sentences.append([ingredient1, ingredient2])\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=8)  # Increase 'workers' to utilize more CPU cores"
      ],
      "metadata": {
        "id": "i7iYEDjNu9Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating graph embeddings"
      ],
      "metadata": {
        "id": "VDlHoTQ-THl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function to filter ingredient nodes in parallel\n",
        "def filter_ingredient_nodes(node, attr):\n",
        "    return node if attr['node_type'] == 'ingredient' else None\n",
        "\n",
        "# Load the knowledge graph\n",
        "flavor_graph = nx.read_graphml('/content/drive/My Drive/ERP/knowledge_graph.graphml') # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Parallelize the filtering process\n",
        "ingredient_nodes = Parallel(n_jobs=-1)(delayed(filter_ingredient_nodes)(n, attr) for n, attr in flavor_graph.nodes(data=True))\n",
        "ingredient_nodes = [node for node in ingredient_nodes if node is not None]\n",
        "\n",
        "# Create a subgraph with only ingredient nodes\n",
        "flavor_graph = flavor_graph.subgraph(ingredient_nodes)\n",
        "\n",
        "# Generate Node2Vec embeddings considering edge weights\n",
        "node2vec = Node2Vec(flavor_graph, dimensions=100, walk_length=30, num_walks=200, workers=16, weight_key='weight')\n",
        "graph_model = node2vec.fit(window=10, min_count=1, batch_words=128)\n",
        "\n",
        "# Generate graph embeddings for the ingredients\n",
        "graph_embeddings = {str(node): graph_model.wv[str(node)] for node in flavor_graph.nodes()}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "f67a139d927f4f8bb4a8c05998928ffd",
            "4c9af84ac1bb443da16068e90254dc4b",
            "388472b0ae104320924c45cc999496cf",
            "5376a37b9b934e869eced9466a97d100",
            "10a4d2a201474fe6a87af8afdd867e03",
            "251e9559272c4a79b90bd440092b4493",
            "44dfc7b6e8f541e2beb6bfbf07516de9",
            "134a2286c92a4c9c8ec08dbcd09a45ce",
            "7cca2cfb40614b9fb296efc72376420d",
            "4411ae07146b4acc8063f203e2614e4a",
            "5db41fe0c94e4583970d7a5d76e5019d"
          ]
        },
        "id": "CzTKyoJ8vFYv",
        "outputId": "f022fcbb-744a-40be-addf-1878659ca78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/6651 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f67a139d927f4f8bb4a8c05998928ffd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined embeddinngs"
      ],
      "metadata": {
        "id": "XpHfgyPGTLI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_combined_embedding(ingredient, text_embeddings, graph_embeddings):\n",
        "    # Get text embedding\n",
        "    if ingredient in text_embeddings:\n",
        "        text_embedding = text_embeddings[ingredient]\n",
        "    else:\n",
        "        text_embedding = np.zeros(100)\n",
        "\n",
        "    # Get graph embedding\n",
        "    if ingredient in graph_embeddings:\n",
        "        graph_embedding = graph_embeddings[ingredient]\n",
        "    else:\n",
        "        graph_embedding = np.zeros(100)\n",
        "\n",
        "    # Combine embeddings by concatenation\n",
        "    combined_embedding = np.concatenate((text_embedding, graph_embedding))\n",
        "\n",
        "    return combined_embedding"
      ],
      "metadata": {
        "id": "rXyqVkvQvJJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining and running the neural network"
      ],
      "metadata": {
        "id": "LbIi7BsHTP2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Combine embeddings for training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ing1 = row['ingredient1']\n",
        "    ing2 = row['ingredient2']\n",
        "    combined_embedding1 = get_combined_embedding(ing1, model.wv, graph_embeddings)\n",
        "    combined_embedding2 = get_combined_embedding(ing2, model.wv, graph_embeddings)\n",
        "\n",
        "    train_data.append(combined_embedding1)\n",
        "    train_labels.append(combined_embedding2)\n",
        "\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network\n",
        "class CombinedNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CombinedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "nn_model = CombinedNN(input_dim=200, output_dim=200)  # Combined embedding dimension is 200 (100 + 100)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(300):\n",
        "    nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = nn_model(train_data)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2B3XPo_vQnf",
        "outputId": "277bf64a-6118-4552-a504-797735f198e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-dfc8e49b499a>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  train_data = torch.tensor(train_data, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8111301064491272\n",
            "Epoch 2, Loss: 0.7933351993560791\n",
            "Epoch 3, Loss: 0.7788990139961243\n",
            "Epoch 4, Loss: 0.7673404812812805\n",
            "Epoch 5, Loss: 0.7579715251922607\n",
            "Epoch 6, Loss: 0.7497022151947021\n",
            "Epoch 7, Loss: 0.743458092212677\n",
            "Epoch 8, Loss: 0.7376641035079956\n",
            "Epoch 9, Loss: 0.7331604957580566\n",
            "Epoch 10, Loss: 0.7289150953292847\n",
            "Epoch 11, Loss: 0.7251368165016174\n",
            "Epoch 12, Loss: 0.7217347025871277\n",
            "Epoch 13, Loss: 0.7187614440917969\n",
            "Epoch 14, Loss: 0.7158350348472595\n",
            "Epoch 15, Loss: 0.7130448818206787\n",
            "Epoch 16, Loss: 0.7102556824684143\n",
            "Epoch 17, Loss: 0.7079573273658752\n",
            "Epoch 18, Loss: 0.7053999304771423\n",
            "Epoch 19, Loss: 0.7034192085266113\n",
            "Epoch 20, Loss: 0.7009631395339966\n",
            "Epoch 21, Loss: 0.6988677978515625\n",
            "Epoch 22, Loss: 0.6971023082733154\n",
            "Epoch 23, Loss: 0.6952605843544006\n",
            "Epoch 24, Loss: 0.6932078003883362\n",
            "Epoch 25, Loss: 0.691659152507782\n",
            "Epoch 26, Loss: 0.6898471117019653\n",
            "Epoch 27, Loss: 0.6881065964698792\n",
            "Epoch 28, Loss: 0.6862443685531616\n",
            "Epoch 29, Loss: 0.6852736473083496\n",
            "Epoch 30, Loss: 0.683684766292572\n",
            "Epoch 31, Loss: 0.6823733448982239\n",
            "Epoch 32, Loss: 0.6808838844299316\n",
            "Epoch 33, Loss: 0.6798035502433777\n",
            "Epoch 34, Loss: 0.6784458756446838\n",
            "Epoch 35, Loss: 0.6776279807090759\n",
            "Epoch 36, Loss: 0.6764407157897949\n",
            "Epoch 37, Loss: 0.6752076148986816\n",
            "Epoch 38, Loss: 0.6738167405128479\n",
            "Epoch 39, Loss: 0.6731405258178711\n",
            "Epoch 40, Loss: 0.6720430850982666\n",
            "Epoch 41, Loss: 0.6708405613899231\n",
            "Epoch 42, Loss: 0.6699110269546509\n",
            "Epoch 43, Loss: 0.668999433517456\n",
            "Epoch 44, Loss: 0.6683202981948853\n",
            "Epoch 45, Loss: 0.6671162843704224\n",
            "Epoch 46, Loss: 0.6662132740020752\n",
            "Epoch 47, Loss: 0.6655622124671936\n",
            "Epoch 48, Loss: 0.6645472645759583\n",
            "Epoch 49, Loss: 0.6640579700469971\n",
            "Epoch 50, Loss: 0.6629006266593933\n",
            "Epoch 51, Loss: 0.6623322367668152\n",
            "Epoch 52, Loss: 0.6613926887512207\n",
            "Epoch 53, Loss: 0.6606200337409973\n",
            "Epoch 54, Loss: 0.6603621244430542\n",
            "Epoch 55, Loss: 0.6588833332061768\n",
            "Epoch 56, Loss: 0.658764123916626\n",
            "Epoch 57, Loss: 0.6579917073249817\n",
            "Epoch 58, Loss: 0.6568047404289246\n",
            "Epoch 59, Loss: 0.6563596725463867\n",
            "Epoch 60, Loss: 0.6559516191482544\n",
            "Epoch 61, Loss: 0.6555216908454895\n",
            "Epoch 62, Loss: 0.6549810767173767\n",
            "Epoch 63, Loss: 0.6544826626777649\n",
            "Epoch 64, Loss: 0.653283953666687\n",
            "Epoch 65, Loss: 0.652806282043457\n",
            "Epoch 66, Loss: 0.6528010964393616\n",
            "Epoch 67, Loss: 0.6522765159606934\n",
            "Epoch 68, Loss: 0.6518493294715881\n",
            "Epoch 69, Loss: 0.6508599519729614\n",
            "Epoch 70, Loss: 0.6504441499710083\n",
            "Epoch 71, Loss: 0.6493558883666992\n",
            "Epoch 72, Loss: 0.6498411297798157\n",
            "Epoch 73, Loss: 0.6489630341529846\n",
            "Epoch 74, Loss: 0.6483898162841797\n",
            "Epoch 75, Loss: 0.6485338807106018\n",
            "Epoch 76, Loss: 0.6476971507072449\n",
            "Epoch 77, Loss: 0.6475951075553894\n",
            "Epoch 78, Loss: 0.6469843983650208\n",
            "Epoch 79, Loss: 0.646094024181366\n",
            "Epoch 80, Loss: 0.6463852524757385\n",
            "Epoch 81, Loss: 0.6459534168243408\n",
            "Epoch 82, Loss: 0.6454768776893616\n",
            "Epoch 83, Loss: 0.6450319886207581\n",
            "Epoch 84, Loss: 0.644005298614502\n",
            "Epoch 85, Loss: 0.6443529725074768\n",
            "Epoch 86, Loss: 0.6440764665603638\n",
            "Epoch 87, Loss: 0.643570601940155\n",
            "Epoch 88, Loss: 0.6431911587715149\n",
            "Epoch 89, Loss: 0.642967700958252\n",
            "Epoch 90, Loss: 0.6429343223571777\n",
            "Epoch 91, Loss: 0.6420764923095703\n",
            "Epoch 92, Loss: 0.6417322754859924\n",
            "Epoch 93, Loss: 0.641879677772522\n",
            "Epoch 94, Loss: 0.6412096619606018\n",
            "Epoch 95, Loss: 0.6407765746116638\n",
            "Epoch 96, Loss: 0.6403399109840393\n",
            "Epoch 97, Loss: 0.6403497457504272\n",
            "Epoch 98, Loss: 0.6402345895767212\n",
            "Epoch 99, Loss: 0.6398429274559021\n",
            "Epoch 100, Loss: 0.6397997140884399\n",
            "Epoch 101, Loss: 0.6394545435905457\n",
            "Epoch 102, Loss: 0.638577938079834\n",
            "Epoch 103, Loss: 0.6390758752822876\n",
            "Epoch 104, Loss: 0.638763964176178\n",
            "Epoch 105, Loss: 0.6381708383560181\n",
            "Epoch 106, Loss: 0.6380930542945862\n",
            "Epoch 107, Loss: 0.6379703879356384\n",
            "Epoch 108, Loss: 0.6376757025718689\n",
            "Epoch 109, Loss: 0.6375321745872498\n",
            "Epoch 110, Loss: 0.637388288974762\n",
            "Epoch 111, Loss: 0.6371005177497864\n",
            "Epoch 112, Loss: 0.6365900635719299\n",
            "Epoch 113, Loss: 0.6368862390518188\n",
            "Epoch 114, Loss: 0.6362343430519104\n",
            "Epoch 115, Loss: 0.6364795565605164\n",
            "Epoch 116, Loss: 0.636478841304779\n",
            "Epoch 117, Loss: 0.6356853246688843\n",
            "Epoch 118, Loss: 0.6355922818183899\n",
            "Epoch 119, Loss: 0.6356695890426636\n",
            "Epoch 120, Loss: 0.6354324817657471\n",
            "Epoch 121, Loss: 0.6353447437286377\n",
            "Epoch 122, Loss: 0.6346549987792969\n",
            "Epoch 123, Loss: 0.6345593929290771\n",
            "Epoch 124, Loss: 0.6349113583564758\n",
            "Epoch 125, Loss: 0.6344178915023804\n",
            "Epoch 126, Loss: 0.6338471174240112\n",
            "Epoch 127, Loss: 0.6337902545928955\n",
            "Epoch 128, Loss: 0.6335347890853882\n",
            "Epoch 129, Loss: 0.6336707472801208\n",
            "Epoch 130, Loss: 0.6337720155715942\n",
            "Epoch 131, Loss: 0.633568286895752\n",
            "Epoch 132, Loss: 0.6336698532104492\n",
            "Epoch 133, Loss: 0.6333638429641724\n",
            "Epoch 134, Loss: 0.6327559947967529\n",
            "Epoch 135, Loss: 0.6326591968536377\n",
            "Epoch 136, Loss: 0.6324765086174011\n",
            "Epoch 137, Loss: 0.6324607729911804\n",
            "Epoch 138, Loss: 0.6323127746582031\n",
            "Epoch 139, Loss: 0.6321457028388977\n",
            "Epoch 140, Loss: 0.6323773264884949\n",
            "Epoch 141, Loss: 0.6321495771408081\n",
            "Epoch 142, Loss: 0.6317476630210876\n",
            "Epoch 143, Loss: 0.6320932507514954\n",
            "Epoch 144, Loss: 0.6317816972732544\n",
            "Epoch 145, Loss: 0.6315484642982483\n",
            "Epoch 146, Loss: 0.631443977355957\n",
            "Epoch 147, Loss: 0.6311103701591492\n",
            "Epoch 148, Loss: 0.6314499378204346\n",
            "Epoch 149, Loss: 0.6306874752044678\n",
            "Epoch 150, Loss: 0.6307603120803833\n",
            "Epoch 151, Loss: 0.6308140754699707\n",
            "Epoch 152, Loss: 0.6306605935096741\n",
            "Epoch 153, Loss: 0.630516529083252\n",
            "Epoch 154, Loss: 0.6305199265480042\n",
            "Epoch 155, Loss: 0.6304031014442444\n",
            "Epoch 156, Loss: 0.630075991153717\n",
            "Epoch 157, Loss: 0.6303473711013794\n",
            "Epoch 158, Loss: 0.6296995282173157\n",
            "Epoch 159, Loss: 0.6298710703849792\n",
            "Epoch 160, Loss: 0.6300788521766663\n",
            "Epoch 161, Loss: 0.6297811269760132\n",
            "Epoch 162, Loss: 0.6294077634811401\n",
            "Epoch 163, Loss: 0.6295959949493408\n",
            "Epoch 164, Loss: 0.6292737126350403\n",
            "Epoch 165, Loss: 0.6293697357177734\n",
            "Epoch 166, Loss: 0.6295660138130188\n",
            "Epoch 167, Loss: 0.629332959651947\n",
            "Epoch 168, Loss: 0.6290936470031738\n",
            "Epoch 169, Loss: 0.6296342015266418\n",
            "Epoch 170, Loss: 0.6292690634727478\n",
            "Epoch 171, Loss: 0.628960132598877\n",
            "Epoch 172, Loss: 0.6281871795654297\n",
            "Epoch 173, Loss: 0.6284319758415222\n",
            "Epoch 174, Loss: 0.6284640431404114\n",
            "Epoch 175, Loss: 0.6284330487251282\n",
            "Epoch 176, Loss: 0.6284028887748718\n",
            "Epoch 177, Loss: 0.6281353831291199\n",
            "Epoch 178, Loss: 0.6288971900939941\n",
            "Epoch 179, Loss: 0.628810703754425\n",
            "Epoch 180, Loss: 0.628027081489563\n",
            "Epoch 181, Loss: 0.6279205083847046\n",
            "Epoch 182, Loss: 0.6274291276931763\n",
            "Epoch 183, Loss: 0.627931535243988\n",
            "Epoch 184, Loss: 0.6278454661369324\n",
            "Epoch 185, Loss: 0.6278970837593079\n",
            "Epoch 186, Loss: 0.6273399591445923\n",
            "Epoch 187, Loss: 0.6269893050193787\n",
            "Epoch 188, Loss: 0.6274110674858093\n",
            "Epoch 189, Loss: 0.6270080804824829\n",
            "Epoch 190, Loss: 0.6272079348564148\n",
            "Epoch 191, Loss: 0.6274368762969971\n",
            "Epoch 192, Loss: 0.626820981502533\n",
            "Epoch 193, Loss: 0.6268001198768616\n",
            "Epoch 194, Loss: 0.6270348429679871\n",
            "Epoch 195, Loss: 0.6274523138999939\n",
            "Epoch 196, Loss: 0.6271622180938721\n",
            "Epoch 197, Loss: 0.6268818974494934\n",
            "Epoch 198, Loss: 0.6257348656654358\n",
            "Epoch 199, Loss: 0.6268227696418762\n",
            "Epoch 200, Loss: 0.6269549131393433\n",
            "Epoch 201, Loss: 0.626811146736145\n",
            "Epoch 202, Loss: 0.6260722875595093\n",
            "Epoch 203, Loss: 0.6261730194091797\n",
            "Epoch 204, Loss: 0.6263799667358398\n",
            "Epoch 205, Loss: 0.6263887286186218\n",
            "Epoch 206, Loss: 0.6261141300201416\n",
            "Epoch 207, Loss: 0.6263391971588135\n",
            "Epoch 208, Loss: 0.6262949705123901\n",
            "Epoch 209, Loss: 0.6263370513916016\n",
            "Epoch 210, Loss: 0.62550288438797\n",
            "Epoch 211, Loss: 0.62626713514328\n",
            "Epoch 212, Loss: 0.6257581114768982\n",
            "Epoch 213, Loss: 0.6256682872772217\n",
            "Epoch 214, Loss: 0.6261287927627563\n",
            "Epoch 215, Loss: 0.6250225901603699\n",
            "Epoch 216, Loss: 0.6253097057342529\n",
            "Epoch 217, Loss: 0.6261164546012878\n",
            "Epoch 218, Loss: 0.625236451625824\n",
            "Epoch 219, Loss: 0.6251850128173828\n",
            "Epoch 220, Loss: 0.6252346634864807\n",
            "Epoch 221, Loss: 0.625287652015686\n",
            "Epoch 222, Loss: 0.6250288486480713\n",
            "Epoch 223, Loss: 0.6256254315376282\n",
            "Epoch 224, Loss: 0.6254068613052368\n",
            "Epoch 225, Loss: 0.6253938674926758\n",
            "Epoch 226, Loss: 0.6250487565994263\n",
            "Epoch 227, Loss: 0.6253514289855957\n",
            "Epoch 228, Loss: 0.6250171661376953\n",
            "Epoch 229, Loss: 0.624728262424469\n",
            "Epoch 230, Loss: 0.6253830790519714\n",
            "Epoch 231, Loss: 0.6248724460601807\n",
            "Epoch 232, Loss: 0.624369204044342\n",
            "Epoch 233, Loss: 0.6244522333145142\n",
            "Epoch 234, Loss: 0.6252641081809998\n",
            "Epoch 235, Loss: 0.6243667602539062\n",
            "Epoch 236, Loss: 0.6244449615478516\n",
            "Epoch 237, Loss: 0.6241775751113892\n",
            "Epoch 238, Loss: 0.6244193315505981\n",
            "Epoch 239, Loss: 0.624397337436676\n",
            "Epoch 240, Loss: 0.6243674755096436\n",
            "Epoch 241, Loss: 0.6246508955955505\n",
            "Epoch 242, Loss: 0.6246045827865601\n",
            "Epoch 243, Loss: 0.6244354248046875\n",
            "Epoch 244, Loss: 0.6244074106216431\n",
            "Epoch 245, Loss: 0.6245361566543579\n",
            "Epoch 246, Loss: 0.6246415376663208\n",
            "Epoch 247, Loss: 0.624442994594574\n",
            "Epoch 248, Loss: 0.6241950392723083\n",
            "Epoch 249, Loss: 0.6246811151504517\n",
            "Epoch 250, Loss: 0.6238545775413513\n",
            "Epoch 251, Loss: 0.623795211315155\n",
            "Epoch 252, Loss: 0.6235125064849854\n",
            "Epoch 253, Loss: 0.6236127018928528\n",
            "Epoch 254, Loss: 0.6237146258354187\n",
            "Epoch 255, Loss: 0.6237578392028809\n",
            "Epoch 256, Loss: 0.6236509680747986\n",
            "Epoch 257, Loss: 0.6233254075050354\n",
            "Epoch 258, Loss: 0.6239038109779358\n",
            "Epoch 259, Loss: 0.6238163113594055\n",
            "Epoch 260, Loss: 0.6236261129379272\n",
            "Epoch 261, Loss: 0.6234399676322937\n",
            "Epoch 262, Loss: 0.6241476535797119\n",
            "Epoch 263, Loss: 0.6235880255699158\n",
            "Epoch 264, Loss: 0.6231493353843689\n",
            "Epoch 265, Loss: 0.6233084797859192\n",
            "Epoch 266, Loss: 0.6233459711074829\n",
            "Epoch 267, Loss: 0.6240096688270569\n",
            "Epoch 268, Loss: 0.6233516931533813\n",
            "Epoch 269, Loss: 0.6227390766143799\n",
            "Epoch 270, Loss: 0.6230306029319763\n",
            "Epoch 271, Loss: 0.6231569647789001\n",
            "Epoch 272, Loss: 0.623430073261261\n",
            "Epoch 273, Loss: 0.6229231953620911\n",
            "Epoch 274, Loss: 0.6229942440986633\n",
            "Epoch 275, Loss: 0.6231803297996521\n",
            "Epoch 276, Loss: 0.6226549744606018\n",
            "Epoch 277, Loss: 0.6225225329399109\n",
            "Epoch 278, Loss: 0.6224808096885681\n",
            "Epoch 279, Loss: 0.6229261159896851\n",
            "Epoch 280, Loss: 0.6229826807975769\n",
            "Epoch 281, Loss: 0.6224862933158875\n",
            "Epoch 282, Loss: 0.6221097111701965\n",
            "Epoch 283, Loss: 0.623086154460907\n",
            "Epoch 284, Loss: 0.622616708278656\n",
            "Epoch 285, Loss: 0.6227714419364929\n",
            "Epoch 286, Loss: 0.6217718720436096\n",
            "Epoch 287, Loss: 0.6222212314605713\n",
            "Epoch 288, Loss: 0.6225899457931519\n",
            "Epoch 289, Loss: 0.6220712065696716\n",
            "Epoch 290, Loss: 0.6225400567054749\n",
            "Epoch 291, Loss: 0.6227405667304993\n",
            "Epoch 292, Loss: 0.6222977638244629\n",
            "Epoch 293, Loss: 0.6225643157958984\n",
            "Epoch 294, Loss: 0.6223433017730713\n",
            "Epoch 295, Loss: 0.6217887997627258\n",
            "Epoch 296, Loss: 0.6216503381729126\n",
            "Epoch 297, Loss: 0.6225746870040894\n",
            "Epoch 298, Loss: 0.6222637295722961\n",
            "Epoch 299, Loss: 0.621965765953064\n",
            "Epoch 300, Loss: 0.6217395067214966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "ZtTwXnzkTURU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import jellyfish\n",
        "\n",
        "# Prepare validation data for the first 500 entries\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    if len(val_data) >= 500:\n",
        "        break\n",
        "    ing1 = row['ingredient1']\n",
        "    combined_embedding = get_combined_embedding(ing1, model.wv, graph_embeddings)\n",
        "\n",
        "    val_data.append(combined_embedding)\n",
        "    val_labels.append(row['ingredient2'])\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
        "val_predictions = nn_model(val_data).detach().numpy()\n",
        "\n",
        "# Function to find the top N most similar ingredients based on cosine similarity\n",
        "def find_top_similar_ingredients(predicted_embedding, combined_embeddings, top_n=10):\n",
        "    similarities = {}\n",
        "    for ingredient, embedding in combined_embeddings.items():\n",
        "        similarity = cosine_similarity(predicted_embedding.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
        "        similarities[ingredient] = similarity\n",
        "    sorted_ingredients = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [ingredient for ingredient, similarity in sorted_ingredients[:top_n]]\n",
        "\n",
        "# Function to calculate metrics with Jaro-Winkler similarity threshold\n",
        "def calculate_metrics(predictions, ground_truths, combined_embeddings, top_n=10, threshold=0.8):\n",
        "    mrr, hit_1, hit_3, hit_10 = 0.0, 0.0, 0.0, 0.0\n",
        "    total = len(ground_truths)\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        top_similar = find_top_similar_ingredients(pred, combined_embeddings, top_n=top_n)\n",
        "        for rank, candidate in enumerate(top_similar, start=1):\n",
        "            sim = jellyfish.jaro_winkler_similarity(gt, candidate)\n",
        "            if sim >= threshold:\n",
        "                mrr += 1.0 / rank\n",
        "                if rank == 1:\n",
        "                    hit_1 += 1.0\n",
        "                if rank <= 3:\n",
        "                    hit_3 += 1.0\n",
        "                if rank <= 10:\n",
        "                    hit_10 += 1.0\n",
        "                break\n",
        "\n",
        "    mrr /= total\n",
        "    hit_1 /= total\n",
        "    hit_3 /= total\n",
        "    hit_10 /= total\n",
        "    return mrr, hit_1, hit_3, hit_10\n",
        "\n",
        "# Calculate metrics for the first 500 entries of the validation set\n",
        "val_labels_str = val_labels  # Assuming labels are ingredient names\n",
        "combined_embeddings = {ingredient: get_combined_embedding(ingredient, model.wv, graph_embeddings) for ingredient in model.wv.index_to_key}\n",
        "mrr, hit_1, hit_3, hit_10 = calculate_metrics(val_predictions, val_labels_str, combined_embeddings)\n",
        "\n",
        "print(f\"MRR: {mrr:.4f}, Hit@1: {hit_1:.4f}, Hit@3: {hit_3:.4f}, Hit@10: {hit_10:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HAv3RHXvaGw",
        "outputId": "28f6556b-d873-41e2-deb1-461bb52f6073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR: 0.1412, Hit@1: 0.0880, Hit@3: 0.1640, Hit@10: 0.2920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the graph model for future use"
      ],
      "metadata": {
        "id": "zRN8AH-yTYoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Save the graph embeddings\n",
        "with open('graph_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(graph_embeddings, f)\n",
        "\n",
        "# Save the Node2Vec model\n",
        "graph_model.save('/content/drive/My Drive/ERP/node2vec_model_actual.model')"
      ],
      "metadata": {
        "id": "ccyXFli-Ibkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the graph embeddings\n",
        "with open('graph_embeddings.pkl', 'rb') as f:\n",
        "    graph_embeddings = pickle.load(f)\n",
        "\n",
        "# Load the Node2Vec model\n",
        "graph_model = Word2Vec.load('/content/drive/My Drive/ERP/node2vec_model_actual.model')\n",
        "\n",
        "# Now you can use the loaded graph embeddings and model as needed"
      ],
      "metadata": {
        "id": "koG5dbCIIgIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}