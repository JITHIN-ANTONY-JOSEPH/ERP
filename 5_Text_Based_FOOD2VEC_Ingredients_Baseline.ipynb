{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "hJagcVzlum0l"
      ],
      "authorship_tag": "ABX9TyPxXz9chGfvVZjU5czWnDD0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JITHIN-ANTONY-JOSEPH/ERP_11358080/blob/main/5_Text_Based_FOOD2VEC_Ingredients_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to connect to Google Drive"
      ],
      "metadata": {
        "id": "hJagcVzlum0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Sx9nAhOY2W",
        "outputId": "97664d1e-22c3-4b50-f317-0ec90740b4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries and required datasets"
      ],
      "metadata": {
        "id": "LzCrq3XDuqAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import networkx as nx\n",
        "from gensim.models import Word2Vec\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import jellyfish"
      ],
      "metadata": {
        "id": "s5ZyhrN6OeG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the flavor graph\n",
        "flavor_graph = nx.read_graphml('/content/drive/My Drive/ERP/knowledge_graph.graphml') # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Extract ingredient nodes from the flavor graph\n",
        "valid_ingredients = {n for n, attr in flavor_graph.nodes(data=True) if attr['node_type'] == 'ingredient'}\n",
        "\n",
        "# Load the Recipe1M processed dataset\n",
        "with open('/content/drive/My Drive/ERP/modified_Processed_Layer1.json', 'r') as file: # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "    recipe1m_data = [json.loads(line) for line in file]"
      ],
      "metadata": {
        "id": "W1SSS85JOj-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "recipe1m_df = pd.DataFrame(recipe1m_data)\n",
        "recipe1m_df['ingredients'] = recipe1m_df['processed_ingredients'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Load the substitution pairs\n",
        "substitution_pairs_df = pd.read_csv('/content/drive/My Drive/ERP/Recipe1MSubs_full.csv') # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Merge the datasets based on recipe_id (substitution_pairs_df) and id (recipe1m_df)\n",
        "merged_df = pd.merge(recipe1m_df, substitution_pairs_df, left_on='id', right_on='recipe_id')"
      ],
      "metadata": {
        "id": "_bKQI2tbOomg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to generate Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "tX5oJfyHuxIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Generate Enhanced Ingredient Lists\n",
        "# Initialize an empty list to store the enhanced sentences\n",
        "enhanced_sentences = []\n",
        "\n",
        "# Iterate over each recipe in merged_df\n",
        "for _, recipe_row in merged_df.iterrows():\n",
        "    recipe_id = recipe_row['id']\n",
        "    ingredients = recipe_row['processed_ingredients']\n",
        "\n",
        "    # Initialize an empty set to store valid ingredients\n",
        "    ingredient_set = set()\n",
        "\n",
        "    # Split each ingredient description into individual words and check each word\n",
        "    for ingredient in ingredients:\n",
        "        words = ingredient.split()\n",
        "        valid_words = [word for word in words if word in valid_ingredients]\n",
        "        ingredient_set.update(valid_words)\n",
        "\n",
        "    # Find substitution pairs for the current recipe\n",
        "    recipe_subs = substitution_pairs_df[substitution_pairs_df['recipe_id'] == recipe_id]\n",
        "\n",
        "    # Add valid substitution pairs to the ingredient set\n",
        "    for _, sub_row in recipe_subs.iterrows():\n",
        "        ingredient1 = sub_row['ingredient1']\n",
        "        ingredient2 = sub_row['ingredient2']\n",
        "        if ingredient1 in valid_ingredients and ingredient2 in valid_ingredients:\n",
        "            ingredient_set.update([ingredient1, ingredient2])\n",
        "\n",
        "    # Convert the set back to a list and add to enhanced sentences\n",
        "    enhanced_sentences.append(list(ingredient_set))\n"
      ],
      "metadata": {
        "id": "fxbScnHKOqdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train the Word2Vec Model\n",
        "# Train Food2Vec model on enhanced sentences\n",
        "food2vec_model = Word2Vec(enhanced_sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "Wbgu5mfXOsXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wnwDJsF6O92h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "RrklnDJbu4EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate Predictions and Evaluate\n",
        "# Function to generate predictions based on cosine similarity\n",
        "def generate_similarity_predictions(ingredient, model, top_k=10):\n",
        "    if ingredient in model.wv:\n",
        "        similarities = model.wv.most_similar(ingredient, topn=top_k)\n",
        "        return [item[0] for item in similarities]\n",
        "    return []\n",
        "\n",
        "# Optimized function to calculate metrics using Jaro-Winkler similarity\n",
        "def calculate_metrics(predictions, ground_truths, threshold=0.8):\n",
        "    mrr, hit_1, hit_3, hit_10 = 0.0, 0.0, 0.0, 0.0\n",
        "    total = len(ground_truths)\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        for i, candidate in enumerate(pred):\n",
        "            sim = jellyfish.jaro_winkler_similarity(gt, candidate)\n",
        "            if sim >= threshold:\n",
        "                rank = i + 1\n",
        "                mrr += 1.0 / rank\n",
        "                if rank == 1:\n",
        "                    hit_1 += 1.0\n",
        "                if rank <= 3:\n",
        "                    hit_3 += 1.0\n",
        "                if rank <= 10:\n",
        "                    hit_10 += 1.0\n",
        "                break\n",
        "\n",
        "    mrr /= total\n",
        "    hit_1 /= total\n",
        "    hit_3 /= total\n",
        "    hit_10 /= total\n",
        "    return mrr, hit_1, hit_3, hit_10\n",
        "\n",
        "# Function to generate predictions for a batch of ingredients\n",
        "def batch_generate_predictions(batch, model):\n",
        "    predictions = []\n",
        "    for ingredient in batch['ingredient1']:\n",
        "        predictions.append(generate_similarity_predictions(ingredient, model))\n",
        "    return predictions\n",
        "\n",
        "# Extract ground truths from the validation set\n",
        "val_ingredient1 = substitution_pairs_df['ingredient1'].tolist()\n",
        "val_ground_truths = substitution_pairs_df['ingredient2'].tolist()\n",
        "\n",
        "# Split validation pairs into batches\n",
        "num_batches = 8  # Adjust based on your CPU cores\n",
        "batches = np.array_split(substitution_pairs_df, num_batches)\n",
        "\n",
        "# Use multiprocessing to generate predictions faster\n",
        "with ProcessPoolExecutor(max_workers=num_batches) as executor:\n",
        "    results = list(executor.map(batch_generate_predictions, batches, [food2vec_model] * num_batches))\n",
        "\n",
        "# Flatten the list of results\n",
        "val_predictions = [item for sublist in results for item in sublist]\n",
        "\n",
        "# Calculate and print the metrics\n",
        "mrr, hit_1, hit_3, hit_10 = calculate_metrics(val_predictions, val_ground_truths, threshold=0.8)\n",
        "print(f\"Word2Vec: MRR: {mrr:.4f}, Hit@1: {hit_1:.4f}, Hit@3: {hit_3:.4f}, Hit@10: {hit_10:.4f}\")\n",
        "\n",
        "# Check substitutions for \"beef\"\n",
        "example_ingredient = 'beef'\n",
        "substitutions = generate_similarity_predictions(example_ingredient, food2vec_model)\n",
        "print(f\"Top substitutions for '{example_ingredient}':\")\n",
        "for substitution in substitutions:\n",
        "    print(substitution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0p6Bli4OuPw",
        "outputId": "05fd8323-57a8-49ef-c338-50de4916187d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec: MRR: 0.0438, Hit@1: 0.0204, Hit@3: 0.0480, Hit@10: 0.1180\n",
            "Top substitutions for 'beef':\n",
            "cooked_rice\n",
            "ground_pork\n",
            "small_onion\n",
            "crushed_tomato\n",
            "chili_powder\n",
            "canned_tomato\n",
            "saltine\n",
            "mushroom\n",
            "pea\n",
            "beef_broth\n"
          ]
        }
      ]
    }
  ]
}