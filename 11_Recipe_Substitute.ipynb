{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPwI0P9yJHFhLxNqs6Ezv6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JITHIN-ANTONY-JOSEPH/ERP_11358080/blob/main/11_Recipe_Substitute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMUjRWRhnOrR",
        "outputId": "9e0c487f-5799-4e58-c32b-c313a351f45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# Load the main dataset\n",
        "with open('/content/drive/My Drive/ERP/modified_Processed_Layer1.json', 'r') as file:\n",
        "    recipe1m_data = [json.loads(line) for line in file]\n",
        "\n",
        "recipe1m_df = pd.DataFrame(recipe1m_data)\n",
        "\n",
        "# Load the substitution pairs\n",
        "substitution_pairs_df = pd.read_csv('/content/drive/My Drive/ERP/Recipe1MSubs_full.csv')\n",
        "\n",
        "# Load flavor graph\n",
        "flavorgraph_df = pd.read_csv('/content/drive/My Drive/ERP/Dataset/nodes_191120.csv')  # replace with your actual path\n",
        "\n",
        "# Example ingredient list for NER-like extraction (replace with your own comprehensive list or use NER model)\n",
        "ingredient_list = set(flavorgraph_df[flavorgraph_df['node_type'] == 'ingredient']['name'].dropna().unique())\n",
        "\n",
        "# Load the existing graph embeddings from file\n",
        "with open('/content/drive/My Drive/ERP/MODEL_BEST_NUMBERS/graph_embeddings.pkl', 'rb') as f:\n",
        "    graph_embeddings = pickle.load(f)\n",
        "\n",
        "# Generate the valid substitutes dictionary\n",
        "valid_substitutes = {}\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ing1 = row['ingredient1']\n",
        "    ing2 = row['ingredient2']\n",
        "    if ing1 not in valid_substitutes:\n",
        "        valid_substitutes[ing1] = set()\n",
        "    valid_substitutes[ing1].add(ing2)\n",
        "\n",
        "# Generate random embeddings for recipe IDs\n",
        "recipe_id_list = recipe1m_df['id'].unique()  # Get all unique recipe IDs\n",
        "recipe_id_embeddings = {recipe_id: np.random.rand(100) for recipe_id in recipe_id_list}  # Example: 100-dimensional embeddings\n",
        "\n",
        "# Function to extract ingredients from instructions\n",
        "def extract_ingredients_from_instructions(instructions, ingredient_list):\n",
        "    extracted_ingredients = []\n",
        "    for instruction in instructions:\n",
        "        words = instruction.split()\n",
        "        for word in words:\n",
        "            if word in ingredient_list:\n",
        "                extracted_ingredients.append(word)\n",
        "    return extracted_ingredients\n",
        "\n",
        "# Apply the extraction function\n",
        "recipe1m_df['extracted_ingredients'] = recipe1m_df['processed_instructions'].apply(\n",
        "    lambda instructions: extract_ingredients_from_instructions(instructions, ingredient_list) if isinstance(instructions, list) else []\n",
        ")\n",
        "\n",
        "# Prepare sentences for training\n",
        "sentences = recipe1m_df['extracted_ingredients'].tolist()\n",
        "\n",
        "# Add substitution contexts to sentences\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ingredient1 = row['ingredient1']\n",
        "    ingredient2 = row['ingredient2']\n",
        "    sentences.append([ingredient1, ingredient2])\n",
        "\n",
        "# Train the Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=8)\n",
        "\n",
        "# Function to combine text, graph, and recipe ID embeddings\n",
        "def get_combined_embedding(ingredient, recipe_id, text_embeddings, graph_embeddings, recipe_id_embeddings):\n",
        "    # Get text embedding\n",
        "    if ingredient in text_embeddings:\n",
        "        text_embedding = text_embeddings[ingredient]\n",
        "    else:\n",
        "        text_embedding = np.zeros(100)\n",
        "\n",
        "    # Get graph embedding\n",
        "    if ingredient in graph_embeddings:\n",
        "        graph_embedding = graph_embeddings[ingredient]\n",
        "    else:\n",
        "        graph_embedding = np.zeros(100)\n",
        "\n",
        "    # Get recipe ID embedding\n",
        "    if recipe_id in recipe_id_embeddings:\n",
        "        recipe_embedding = recipe_id_embeddings[recipe_id]\n",
        "    else:\n",
        "        recipe_embedding = np.zeros(100)\n",
        "\n",
        "    # Combine embeddings by concatenation\n",
        "    combined_embedding = np.concatenate((text_embedding, graph_embedding, recipe_embedding))\n",
        "\n",
        "    return combined_embedding\n",
        "\n",
        "# Prepare training data with negative samples\n",
        "train_data = []\n",
        "train_labels = []\n",
        "negative_samples = []\n",
        "negative_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ing1 = row['ingredient1']\n",
        "    ing2 = row['ingredient2']\n",
        "    recipe_id = row['recipe_id']  # Assuming each row in the substitution pairs includes a recipe ID\n",
        "\n",
        "    combined_embedding1 = get_combined_embedding(ing1, recipe_id, model.wv, graph_embeddings, recipe_id_embeddings)\n",
        "    combined_embedding2 = get_combined_embedding(ing2, recipe_id, model.wv, graph_embeddings, recipe_id_embeddings)\n",
        "\n",
        "    train_data.append(combined_embedding1)\n",
        "    train_labels.append(combined_embedding2)\n",
        "\n",
        "    # Generate negative samples by excluding valid substitutes\n",
        "    possible_negatives = [\n",
        "        ing for ing in ingredient_list\n",
        "        if ing != ing1 and ing not in valid_substitutes.get(ing1, set())\n",
        "    ]\n",
        "    selected_negatives = random.sample(possible_negatives, min(100, len(possible_negatives)))  # Pick 100 non-substitutes\n",
        "\n",
        "    for neg in selected_negatives:\n",
        "        neg_embedding = get_combined_embedding(neg, recipe_id, model.wv, graph_embeddings, recipe_id_embeddings)\n",
        "        negative_labels.append(neg_embedding)\n",
        "        negative_samples.append(combined_embedding1)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "negative_samples = torch.tensor(negative_samples, dtype=torch.float32)\n",
        "negative_labels = torch.tensor(negative_labels, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network\n",
        "class CombinedNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CombinedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "nn_model = CombinedNN(input_dim=300, output_dim=300)  # Combined embedding dimension is 300 (100 text + 100 graph + 100 recipe)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with negative sampling\n",
        "for epoch in range(50):\n",
        "    nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Positive pair loss\n",
        "    outputs = nn_model(train_data)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "\n",
        "    # Negative pair loss (contrastive or margin-based loss)\n",
        "    neg_outputs = nn_model(negative_samples)\n",
        "    expanded_outputs = outputs.repeat_interleave(100, dim=0)  # Ensure dimensions match for 100 negatives\n",
        "    negative_loss = torch.mean(torch.relu(1.0 - torch.sum(expanded_outputs * neg_outputs, dim=1)))\n",
        "\n",
        "    total_loss = loss + negative_loss\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss.item()}')\n",
        "\n",
        "# Function to extract ingredients from a specific recipe based on recipeID\n",
        "def extract_ingredients_for_recipe(recipe_id, recipe_df, ingredient_list):\n",
        "    recipe_row = recipe_df[recipe_df['id'] == recipe_id]\n",
        "    if recipe_row.empty:\n",
        "        return []\n",
        "    instructions = recipe_row.iloc[0]['processed_instructions']\n",
        "    return extract_ingredients_from_instructions(instructions, ingredient_list) if isinstance(instructions, list) else []\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUtLrQunPJe",
        "outputId": "c132f5db-59cc-45c2-d63d-78534441911d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-eb382693d068>:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  train_data = torch.tensor(train_data, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6585375666618347\n",
            "Epoch 2, Loss: 0.6332034468650818\n",
            "Epoch 3, Loss: 0.6154450178146362\n",
            "Epoch 4, Loss: 0.6027324199676514\n",
            "Epoch 5, Loss: 0.5927829146385193\n",
            "Epoch 6, Loss: 0.5849650502204895\n",
            "Epoch 7, Loss: 0.5780840516090393\n",
            "Epoch 8, Loss: 0.5722802877426147\n",
            "Epoch 9, Loss: 0.5666438937187195\n",
            "Epoch 10, Loss: 0.5614410042762756\n",
            "Epoch 11, Loss: 0.5562489628791809\n",
            "Epoch 12, Loss: 0.551291823387146\n",
            "Epoch 13, Loss: 0.5468875169754028\n",
            "Epoch 14, Loss: 0.5423744916915894\n",
            "Epoch 15, Loss: 0.5383607745170593\n",
            "Epoch 16, Loss: 0.5343826413154602\n",
            "Epoch 17, Loss: 0.530830979347229\n",
            "Epoch 18, Loss: 0.5274160504341125\n",
            "Epoch 19, Loss: 0.5240288376808167\n",
            "Epoch 20, Loss: 0.5213919878005981\n",
            "Epoch 21, Loss: 0.5182521343231201\n",
            "Epoch 22, Loss: 0.5156731605529785\n",
            "Epoch 23, Loss: 0.5132817625999451\n",
            "Epoch 24, Loss: 0.5109080672264099\n",
            "Epoch 25, Loss: 0.5088294148445129\n",
            "Epoch 26, Loss: 0.5069332718849182\n",
            "Epoch 27, Loss: 0.5049417018890381\n",
            "Epoch 28, Loss: 0.5033687949180603\n",
            "Epoch 29, Loss: 0.5016764998435974\n",
            "Epoch 30, Loss: 0.500068724155426\n",
            "Epoch 31, Loss: 0.498727947473526\n",
            "Epoch 32, Loss: 0.49740439653396606\n",
            "Epoch 33, Loss: 0.49593043327331543\n",
            "Epoch 34, Loss: 0.494905948638916\n",
            "Epoch 35, Loss: 0.4937639534473419\n",
            "Epoch 36, Loss: 0.49264177680015564\n",
            "Epoch 37, Loss: 0.4916754364967346\n",
            "Epoch 38, Loss: 0.4905586540699005\n",
            "Epoch 39, Loss: 0.489532470703125\n",
            "Epoch 40, Loss: 0.488876074552536\n",
            "Epoch 41, Loss: 0.48789191246032715\n",
            "Epoch 42, Loss: 0.4869515001773834\n",
            "Epoch 43, Loss: 0.4862822890281677\n",
            "Epoch 44, Loss: 0.4853712320327759\n",
            "Epoch 45, Loss: 0.4845517575740814\n",
            "Epoch 46, Loss: 0.4838842451572418\n",
            "Epoch 47, Loss: 0.48333606123924255\n",
            "Epoch 48, Loss: 0.4823942184448242\n",
            "Epoch 49, Loss: 0.4818863570690155\n",
            "Epoch 50, Loss: 0.48089399933815\n",
            "Ingredient butter not found in recipe your_recipe_id_here.\n",
            "Top substitutes for butter in recipe your_recipe_id_here: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified function to filter valid substitutes for a given recipeID and ingredient\n",
        "def get_valid_substitutes(recipe_id, ingredient, recipe_df, model, graph_embeddings, recipe_id_embeddings, valid_substitutes, top_n=10):\n",
        "    # Extract context ingredients from the recipe\n",
        "    context_ingredients = extract_ingredients_for_recipe(recipe_id, recipe_df, ingredient_list)\n",
        "\n",
        "    if ingredient not in context_ingredients:\n",
        "        print(f\"Ingredient {ingredient} not found in recipe {recipe_id}.\")\n",
        "        return []\n",
        "\n",
        "    # Get combined embedding for the input ingredient\n",
        "    ingredient_embedding = get_combined_embedding(ingredient, recipe_id, model.wv, graph_embeddings, recipe_id_embeddings)\n",
        "\n",
        "    # Get all potential substitutes\n",
        "    potential_substitutes = valid_substitutes.get(ingredient, [])\n",
        "\n",
        "    # Filter substitutes that are in the context of the recipe\n",
        "    potential_substitutes = [sub for sub in potential_substitutes if sub in context_ingredients]\n",
        "\n",
        "    if not potential_substitutes:\n",
        "        print(f\"No valid substitutes found in context for ingredient {ingredient} in recipe {recipe_id}.\")\n",
        "        return []\n",
        "\n",
        "    # Calculate cosine similarity and find the top substitutes\n",
        "    substitutes = []\n",
        "    for sub in potential_substitutes:\n",
        "        sub_embedding = get_combined_embedding(sub, recipe_id, model.wv, graph_embeddings, recipe_id_embeddings)\n",
        "        similarity = cosine_similarity(ingredient_embedding.reshape(1, -1), sub_embedding.reshape(1, -1))[0][0]\n",
        "        substitutes.append((sub, similarity))\n",
        "\n",
        "    substitutes = sorted(substitutes, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    print(substitutes)\n",
        "    return [sub[0] for sub in substitutes]\n",
        "\n",
        "# Example usage\n",
        "recipe_id = \"1c448c5d40\"  # Replace with the actual recipe ID\n",
        "ingredient_to_replace = \"white_chocolate\"  # Replace with the ingredient you want to substitute\n",
        "\n",
        "top_substitutes = get_valid_substitutes(recipe_id, ingredient_to_replace, recipe1m_df, model, graph_embeddings, recipe_id_embeddings, valid_substitutes)\n",
        "\n",
        "print(f\"Top substitutes for {ingredient_to_replace} in recipe {recipe_id}: {top_substitutes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzAkcxedrJ3n",
        "outputId": "17ef7df3-5daa-4e46-b61a-24d516e75213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('milk_chocolate', 0.7684454057091302), ('chocolate', 0.7018237058410502), ('peanut_butter_chip', 0.4894215049097287), ('butterscotch_chip', 0.45943049943650993), ('orange', 0.23705233600884962)]\n",
            "Top substitutes for white_chocolate in recipe 1c448c5d40: ['milk_chocolate', 'chocolate', 'peanut_butter_chip', 'butterscotch_chip', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the path where you want to save the model and other components\n",
        "model_save_path = '/content/drive/My Drive/ERP/RECIPE_SPECIFIC/saved_model.pth'\n",
        "embeddings_save_path = '/content/drive/My Drive/ERP/RECIPE_SPECIFIC/saved_embeddings.pkl'\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save({\n",
        "    'model_state_dict': nn_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': epoch,  # Save the last epoch if you want to resume training later\n",
        "}, model_save_path)\n",
        "\n",
        "# Save the embeddings (text, graph, and recipe ID embeddings) if needed\n",
        "embeddings_data = {\n",
        "    'text_embeddings': model.wv,\n",
        "    'graph_embeddings': graph_embeddings,\n",
        "    'recipe_id_embeddings': recipe_id_embeddings\n",
        "}\n",
        "\n",
        "with open(embeddings_save_path, 'wb') as f:\n",
        "    pickle.dump(embeddings_data, f)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "print(f\"Embeddings saved to {embeddings_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vohWGb5r8eb",
        "outputId": "4a85fed7-a342-4af9-b3c1-ca0f83ae2dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/My Drive/ERP/RECIPE_SPECIFIC/saved_model.pth\n",
            "Embeddings saved to /content/drive/My Drive/ERP/RECIPE_SPECIFIC/saved_embeddings.pkl\n"
          ]
        }
      ]
    }
  ]
}