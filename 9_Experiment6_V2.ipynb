{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNpqlJBbeDScZoi+j6YhGKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95630b52097742fcb2f8e505edc01665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_544deb4ab87e462bb2975b82d75b7ade",
              "IPY_MODEL_95a9b186e22344a1b81444f3ddbad1b0",
              "IPY_MODEL_e4ec23d6c2124005b71f67369aada7cf"
            ],
            "layout": "IPY_MODEL_143366031eb04d3bbd82d600a6632393"
          }
        },
        "544deb4ab87e462bb2975b82d75b7ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ddea237bed5406ca0d8057ce3fe3ae4",
            "placeholder": "​",
            "style": "IPY_MODEL_930653db5eaf4615af8a40417569841e",
            "value": "Computing transition probabilities: 100%"
          }
        },
        "95a9b186e22344a1b81444f3ddbad1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef290cead344d5ba3a782f0d4956975",
            "max": 6651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c03b94ae1d034ee68a25d6c3e6d6435c",
            "value": 6651
          }
        },
        "e4ec23d6c2124005b71f67369aada7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f519fd39ccaa43499c935c8a4e95dd76",
            "placeholder": "​",
            "style": "IPY_MODEL_fef6c55682f14ffd903fe35a2250132c",
            "value": " 6651/6651 [06:44&lt;00:00, 12.79it/s]"
          }
        },
        "143366031eb04d3bbd82d600a6632393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ddea237bed5406ca0d8057ce3fe3ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930653db5eaf4615af8a40417569841e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fef290cead344d5ba3a782f0d4956975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c03b94ae1d034ee68a25d6c3e6d6435c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f519fd39ccaa43499c935c8a4e95dd76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef6c55682f14ffd903fe35a2250132c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JITHIN-ANTONY-JOSEPH/ERP_11358080/blob/main/9_Experiment6_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input : Recipe Instructions\n",
        "### Model : Word2Vec(Text) + Node2Vec(Graph) + Neural Networks(Model) Version 1 -> 1000 epochs"
      ],
      "metadata": {
        "id": "1lsXfn0ucA0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to connect to Google Drive"
      ],
      "metadata": {
        "id": "XVeM3j1nlImt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbADcz6iu1mq",
        "outputId": "b7e81004-ca09-40ec-e02a-5da04168160c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries , loading datasets and pre-processing"
      ],
      "metadata": {
        "id": "qvSnwaWklNoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q6FOAP2Ru5cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flavorgraph_df = pd.read_csv('/content/drive/My Drive/ERP/Dataset/nodes_191120.csv')  # replace with your actual path"
      ],
      "metadata": {
        "id": "5dyUMt6DvSv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import jellyfish\n",
        "from gensim.models import Word2Vec\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Load the main dataset\n",
        "with open('/content/drive/My Drive/ERP/modified_Processed_Layer1.json', 'r') as file:\n",
        "    recipe1m_data = [json.loads(line) for line in file]\n",
        "\n",
        "recipe1m_df = pd.DataFrame(recipe1m_data)\n",
        "\n",
        "# Load the substitution pairs\n",
        "substitution_pairs_df = pd.read_csv('/content/drive/My Drive/ERP/Recipe1MSubs_full.csv')\n",
        "\n",
        "# Merge the datasets based on recipe_id (substitution_pairs_df) and id (recipe1m_df)\n",
        "merged_df = pd.merge(recipe1m_df, substitution_pairs_df, left_on='id', right_on='recipe_id')\n",
        "\n",
        "# Example ingredient list for NER-like extraction (replace with your own comprehensive list or use NER model)\n",
        "ingredient_list = set(flavorgraph_df[flavorgraph_df['node_type'] == 'ingredient']['name'].dropna().unique())\n",
        "\n",
        "# Function to extract ingredients from instructions\n",
        "def extract_ingredients_from_instructions(instructions, ingredient_list):\n",
        "    extracted_ingredients = []\n",
        "    for instruction in instructions:\n",
        "        words = instruction.split()\n",
        "        for word in words:\n",
        "            if word in ingredient_list:\n",
        "                extracted_ingredients.append(word)\n",
        "    return extracted_ingredients\n",
        "\n",
        "# Apply the extraction function\n",
        "recipe1m_df['extracted_ingredients'] = recipe1m_df['processed_instructions'].apply(\n",
        "    lambda instructions: extract_ingredients_from_instructions(instructions, ingredient_list) if isinstance(instructions, list) else []\n",
        ")\n",
        "\n",
        "# Prepare sentences for training\n",
        "sentences = recipe1m_df['extracted_ingredients'].tolist()\n",
        "\n",
        "# Add substitution contexts to sentences\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ingredient1 = row['ingredient1']\n",
        "    ingredient2 = row['ingredient2']\n",
        "    sentences.append([ingredient1, ingredient2])\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=8)  # Increase 'workers' to utilize more CPU cores"
      ],
      "metadata": {
        "id": "i7iYEDjNu9Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIW_OsaCvCju",
        "outputId": "982c04a6-262e-4d9f-c4bf-a9b13faadbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.66.5)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.16.0)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: node2vec\n",
            "Successfully installed node2vec-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating graph embeddings"
      ],
      "metadata": {
        "id": "pNraioSQlUMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function to filter ingredient nodes in parallel\n",
        "def filter_ingredient_nodes(node, attr):\n",
        "    return node if attr['node_type'] == 'ingredient' else None\n",
        "\n",
        "# Load the knowledge graph\n",
        "flavor_graph = nx.read_graphml('/content/drive/My Drive/ERP/knowledge_graph.graphml')\n",
        "\n",
        "# Parallelize the filtering process\n",
        "ingredient_nodes = Parallel(n_jobs=-1)(delayed(filter_ingredient_nodes)(n, attr) for n, attr in flavor_graph.nodes(data=True))\n",
        "ingredient_nodes = [node for node in ingredient_nodes if node is not None]\n",
        "\n",
        "# Create a subgraph with only ingredient nodes\n",
        "flavor_graph = flavor_graph.subgraph(ingredient_nodes)\n",
        "\n",
        "# Generate Node2Vec embeddings considering edge weights\n",
        "node2vec = Node2Vec(flavor_graph, dimensions=100, walk_length=30, num_walks=200, workers=16, weight_key='weight')\n",
        "graph_model = node2vec.fit(window=10, min_count=1, batch_words=128)\n",
        "\n",
        "# Generate graph embeddings for the ingredients\n",
        "graph_embeddings = {str(node): graph_model.wv[str(node)] for node in flavor_graph.nodes()}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "95630b52097742fcb2f8e505edc01665",
            "544deb4ab87e462bb2975b82d75b7ade",
            "95a9b186e22344a1b81444f3ddbad1b0",
            "e4ec23d6c2124005b71f67369aada7cf",
            "143366031eb04d3bbd82d600a6632393",
            "6ddea237bed5406ca0d8057ce3fe3ae4",
            "930653db5eaf4615af8a40417569841e",
            "fef290cead344d5ba3a782f0d4956975",
            "c03b94ae1d034ee68a25d6c3e6d6435c",
            "f519fd39ccaa43499c935c8a4e95dd76",
            "fef6c55682f14ffd903fe35a2250132c"
          ]
        },
        "id": "CzTKyoJ8vFYv",
        "outputId": "589c47a6-ea40-4760-8480-56d6e9184b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/6651 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95630b52097742fcb2f8e505edc01665"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined embeddinngs"
      ],
      "metadata": {
        "id": "7YJlp5YflXVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_combined_embedding(ingredient, text_embeddings, graph_embeddings):\n",
        "    # Get text embedding\n",
        "    if ingredient in text_embeddings:\n",
        "        text_embedding = text_embeddings[ingredient]\n",
        "    else:\n",
        "        text_embedding = np.zeros(100)\n",
        "\n",
        "    # Get graph embedding\n",
        "    if ingredient in graph_embeddings:\n",
        "        graph_embedding = graph_embeddings[ingredient]\n",
        "    else:\n",
        "        graph_embedding = np.zeros(100)\n",
        "\n",
        "    # Combine embeddings by concatenation\n",
        "    combined_embedding = np.concatenate((text_embedding, graph_embedding))\n",
        "\n",
        "    return combined_embedding"
      ],
      "metadata": {
        "id": "rXyqVkvQvJJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining and running the neural network"
      ],
      "metadata": {
        "id": "CyAaAKaMlaY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Combine embeddings for training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ing1 = row['ingredient1']\n",
        "    ing2 = row['ingredient2']\n",
        "    combined_embedding1 = get_combined_embedding(ing1, model.wv, graph_embeddings)\n",
        "    combined_embedding2 = get_combined_embedding(ing2, model.wv, graph_embeddings)\n",
        "\n",
        "    train_data.append(combined_embedding1)\n",
        "    train_labels.append(combined_embedding2)\n",
        "\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network\n",
        "class CombinedNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CombinedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "nn_model = CombinedNN(input_dim=200, output_dim=200)  # Combined embedding dimension is 200 (100 + 100)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = nn_model(train_data)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2B3XPo_vQnf",
        "outputId": "712b3421-8cb6-4933-e00d-84d9b4db4d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-406027802457>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  train_data = torch.tensor(train_data, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8163601160049438\n",
            "Epoch 2, Loss: 0.7971007227897644\n",
            "Epoch 3, Loss: 0.7823312282562256\n",
            "Epoch 4, Loss: 0.7704818844795227\n",
            "Epoch 5, Loss: 0.7598009705543518\n",
            "Epoch 6, Loss: 0.7516212463378906\n",
            "Epoch 7, Loss: 0.7448369264602661\n",
            "Epoch 8, Loss: 0.739045262336731\n",
            "Epoch 9, Loss: 0.7341566681861877\n",
            "Epoch 10, Loss: 0.7295253872871399\n",
            "Epoch 11, Loss: 0.7258496880531311\n",
            "Epoch 12, Loss: 0.722234845161438\n",
            "Epoch 13, Loss: 0.71904456615448\n",
            "Epoch 14, Loss: 0.7158016562461853\n",
            "Epoch 15, Loss: 0.7130066752433777\n",
            "Epoch 16, Loss: 0.710364580154419\n",
            "Epoch 17, Loss: 0.7080751657485962\n",
            "Epoch 18, Loss: 0.7052030563354492\n",
            "Epoch 19, Loss: 0.7030390501022339\n",
            "Epoch 20, Loss: 0.7007731795310974\n",
            "Epoch 21, Loss: 0.6989062428474426\n",
            "Epoch 22, Loss: 0.6967297792434692\n",
            "Epoch 23, Loss: 0.6946645975112915\n",
            "Epoch 24, Loss: 0.6926723718643188\n",
            "Epoch 25, Loss: 0.6908878087997437\n",
            "Epoch 26, Loss: 0.6894578337669373\n",
            "Epoch 27, Loss: 0.6876606345176697\n",
            "Epoch 28, Loss: 0.6862809062004089\n",
            "Epoch 29, Loss: 0.6850026845932007\n",
            "Epoch 30, Loss: 0.6832176446914673\n",
            "Epoch 31, Loss: 0.6816571354866028\n",
            "Epoch 32, Loss: 0.6801593899726868\n",
            "Epoch 33, Loss: 0.6791292428970337\n",
            "Epoch 34, Loss: 0.6779208779335022\n",
            "Epoch 35, Loss: 0.6765530109405518\n",
            "Epoch 36, Loss: 0.675532877445221\n",
            "Epoch 37, Loss: 0.6744760274887085\n",
            "Epoch 38, Loss: 0.6733852028846741\n",
            "Epoch 39, Loss: 0.6721875071525574\n",
            "Epoch 40, Loss: 0.6711960434913635\n",
            "Epoch 41, Loss: 0.6703941226005554\n",
            "Epoch 42, Loss: 0.6692942976951599\n",
            "Epoch 43, Loss: 0.6681983470916748\n",
            "Epoch 44, Loss: 0.6674681305885315\n",
            "Epoch 45, Loss: 0.6663727164268494\n",
            "Epoch 46, Loss: 0.6655625700950623\n",
            "Epoch 47, Loss: 0.6647084355354309\n",
            "Epoch 48, Loss: 0.6636862754821777\n",
            "Epoch 49, Loss: 0.6635082960128784\n",
            "Epoch 50, Loss: 0.662244975566864\n",
            "Epoch 51, Loss: 0.661629855632782\n",
            "Epoch 52, Loss: 0.6609363555908203\n",
            "Epoch 53, Loss: 0.6601449847221375\n",
            "Epoch 54, Loss: 0.659024178981781\n",
            "Epoch 55, Loss: 0.6588725447654724\n",
            "Epoch 56, Loss: 0.6581602692604065\n",
            "Epoch 57, Loss: 0.6568589806556702\n",
            "Epoch 58, Loss: 0.6569696664810181\n",
            "Epoch 59, Loss: 0.6564205884933472\n",
            "Epoch 60, Loss: 0.6557124257087708\n",
            "Epoch 61, Loss: 0.6550273895263672\n",
            "Epoch 62, Loss: 0.654529869556427\n",
            "Epoch 63, Loss: 0.6535006165504456\n",
            "Epoch 64, Loss: 0.6531301736831665\n",
            "Epoch 65, Loss: 0.6528370380401611\n",
            "Epoch 66, Loss: 0.6524345278739929\n",
            "Epoch 67, Loss: 0.6517899036407471\n",
            "Epoch 68, Loss: 0.6510599851608276\n",
            "Epoch 69, Loss: 0.6503145694732666\n",
            "Epoch 70, Loss: 0.649590253829956\n",
            "Epoch 71, Loss: 0.6499226093292236\n",
            "Epoch 72, Loss: 0.6489582657814026\n",
            "Epoch 73, Loss: 0.6488674879074097\n",
            "Epoch 74, Loss: 0.6481696963310242\n",
            "Epoch 75, Loss: 0.6480714082717896\n",
            "Epoch 76, Loss: 0.6476936936378479\n",
            "Epoch 77, Loss: 0.6469064354896545\n",
            "Epoch 78, Loss: 0.6468328833580017\n",
            "Epoch 79, Loss: 0.6461702585220337\n",
            "Epoch 80, Loss: 0.6457104682922363\n",
            "Epoch 81, Loss: 0.6457889080047607\n",
            "Epoch 82, Loss: 0.6452513933181763\n",
            "Epoch 83, Loss: 0.6450678110122681\n",
            "Epoch 84, Loss: 0.6445868015289307\n",
            "Epoch 85, Loss: 0.644204318523407\n",
            "Epoch 86, Loss: 0.643994152545929\n",
            "Epoch 87, Loss: 0.6436732411384583\n",
            "Epoch 88, Loss: 0.6428855061531067\n",
            "Epoch 89, Loss: 0.6427859663963318\n",
            "Epoch 90, Loss: 0.6428498029708862\n",
            "Epoch 91, Loss: 0.6420524716377258\n",
            "Epoch 92, Loss: 0.6417180895805359\n",
            "Epoch 93, Loss: 0.6412898302078247\n",
            "Epoch 94, Loss: 0.6416139006614685\n",
            "Epoch 95, Loss: 0.6408222913742065\n",
            "Epoch 96, Loss: 0.6407135725021362\n",
            "Epoch 97, Loss: 0.6407920122146606\n",
            "Epoch 98, Loss: 0.6400134563446045\n",
            "Epoch 99, Loss: 0.6401432752609253\n",
            "Epoch 100, Loss: 0.6398760676383972\n",
            "Epoch 101, Loss: 0.6394661068916321\n",
            "Epoch 102, Loss: 0.6392614245414734\n",
            "Epoch 103, Loss: 0.6386515498161316\n",
            "Epoch 104, Loss: 0.6380434036254883\n",
            "Epoch 105, Loss: 0.6387717723846436\n",
            "Epoch 106, Loss: 0.638237714767456\n",
            "Epoch 107, Loss: 0.6381555199623108\n",
            "Epoch 108, Loss: 0.6380460858345032\n",
            "Epoch 109, Loss: 0.63816237449646\n",
            "Epoch 110, Loss: 0.6372758150100708\n",
            "Epoch 111, Loss: 0.6372201442718506\n",
            "Epoch 112, Loss: 0.6368244886398315\n",
            "Epoch 113, Loss: 0.6367833614349365\n",
            "Epoch 114, Loss: 0.6365118026733398\n",
            "Epoch 115, Loss: 0.6361876130104065\n",
            "Epoch 116, Loss: 0.6359530091285706\n",
            "Epoch 117, Loss: 0.6356309652328491\n",
            "Epoch 118, Loss: 0.6362130045890808\n",
            "Epoch 119, Loss: 0.6356272101402283\n",
            "Epoch 120, Loss: 0.635575532913208\n",
            "Epoch 121, Loss: 0.6352961659431458\n",
            "Epoch 122, Loss: 0.6351749897003174\n",
            "Epoch 123, Loss: 0.6351544260978699\n",
            "Epoch 124, Loss: 0.63470458984375\n",
            "Epoch 125, Loss: 0.6344963908195496\n",
            "Epoch 126, Loss: 0.6347084641456604\n",
            "Epoch 127, Loss: 0.6344085931777954\n",
            "Epoch 128, Loss: 0.6340938210487366\n",
            "Epoch 129, Loss: 0.6337283253669739\n",
            "Epoch 130, Loss: 0.6336488127708435\n",
            "Epoch 131, Loss: 0.6334831714630127\n",
            "Epoch 132, Loss: 0.6330459117889404\n",
            "Epoch 133, Loss: 0.6333045363426208\n",
            "Epoch 134, Loss: 0.6330875158309937\n",
            "Epoch 135, Loss: 0.6333169341087341\n",
            "Epoch 136, Loss: 0.6324026584625244\n",
            "Epoch 137, Loss: 0.632689893245697\n",
            "Epoch 138, Loss: 0.6327400207519531\n",
            "Epoch 139, Loss: 0.6323103904724121\n",
            "Epoch 140, Loss: 0.6324178576469421\n",
            "Epoch 141, Loss: 0.6323778629302979\n",
            "Epoch 142, Loss: 0.6318880319595337\n",
            "Epoch 143, Loss: 0.6321969628334045\n",
            "Epoch 144, Loss: 0.6316980719566345\n",
            "Epoch 145, Loss: 0.6318979859352112\n",
            "Epoch 146, Loss: 0.6314322352409363\n",
            "Epoch 147, Loss: 0.63161301612854\n",
            "Epoch 148, Loss: 0.6314855813980103\n",
            "Epoch 149, Loss: 0.6314114928245544\n",
            "Epoch 150, Loss: 0.6306272149085999\n",
            "Epoch 151, Loss: 0.6308988928794861\n",
            "Epoch 152, Loss: 0.6308767199516296\n",
            "Epoch 153, Loss: 0.6304023265838623\n",
            "Epoch 154, Loss: 0.6307529807090759\n",
            "Epoch 155, Loss: 0.6311615705490112\n",
            "Epoch 156, Loss: 0.6304859519004822\n",
            "Epoch 157, Loss: 0.630689263343811\n",
            "Epoch 158, Loss: 0.6303471922874451\n",
            "Epoch 159, Loss: 0.6301101446151733\n",
            "Epoch 160, Loss: 0.6299208402633667\n",
            "Epoch 161, Loss: 0.6297122240066528\n",
            "Epoch 162, Loss: 0.6298694014549255\n",
            "Epoch 163, Loss: 0.6293159127235413\n",
            "Epoch 164, Loss: 0.6297025084495544\n",
            "Epoch 165, Loss: 0.6293649673461914\n",
            "Epoch 166, Loss: 0.629458487033844\n",
            "Epoch 167, Loss: 0.6291036009788513\n",
            "Epoch 168, Loss: 0.6289939880371094\n",
            "Epoch 169, Loss: 0.6293123960494995\n",
            "Epoch 170, Loss: 0.6290321946144104\n",
            "Epoch 171, Loss: 0.628788411617279\n",
            "Epoch 172, Loss: 0.6287854909896851\n",
            "Epoch 173, Loss: 0.6291172504425049\n",
            "Epoch 174, Loss: 0.6281731724739075\n",
            "Epoch 175, Loss: 0.6286728978157043\n",
            "Epoch 176, Loss: 0.6287305951118469\n",
            "Epoch 177, Loss: 0.628328263759613\n",
            "Epoch 178, Loss: 0.6286489367485046\n",
            "Epoch 179, Loss: 0.6285122632980347\n",
            "Epoch 180, Loss: 0.6279556751251221\n",
            "Epoch 181, Loss: 0.6280720829963684\n",
            "Epoch 182, Loss: 0.6280539631843567\n",
            "Epoch 183, Loss: 0.6274650692939758\n",
            "Epoch 184, Loss: 0.627930223941803\n",
            "Epoch 185, Loss: 0.6286291480064392\n",
            "Epoch 186, Loss: 0.6273912191390991\n",
            "Epoch 187, Loss: 0.6273642778396606\n",
            "Epoch 188, Loss: 0.6281266212463379\n",
            "Epoch 189, Loss: 0.6277202367782593\n",
            "Epoch 190, Loss: 0.6275368928909302\n",
            "Epoch 191, Loss: 0.6267974376678467\n",
            "Epoch 192, Loss: 0.6271930932998657\n",
            "Epoch 193, Loss: 0.6273900866508484\n",
            "Epoch 194, Loss: 0.6267709732055664\n",
            "Epoch 195, Loss: 0.6272156238555908\n",
            "Epoch 196, Loss: 0.6262856721878052\n",
            "Epoch 197, Loss: 0.6268218159675598\n",
            "Epoch 198, Loss: 0.6268630623817444\n",
            "Epoch 199, Loss: 0.6266619563102722\n",
            "Epoch 200, Loss: 0.6265051960945129\n",
            "Epoch 201, Loss: 0.626895010471344\n",
            "Epoch 202, Loss: 0.6261829137802124\n",
            "Epoch 203, Loss: 0.6268925070762634\n",
            "Epoch 204, Loss: 0.6264479160308838\n",
            "Epoch 205, Loss: 0.6258864402770996\n",
            "Epoch 206, Loss: 0.6263189315795898\n",
            "Epoch 207, Loss: 0.6264371871948242\n",
            "Epoch 208, Loss: 0.6259550452232361\n",
            "Epoch 209, Loss: 0.6265634298324585\n",
            "Epoch 210, Loss: 0.6254567503929138\n",
            "Epoch 211, Loss: 0.6251178979873657\n",
            "Epoch 212, Loss: 0.6258504390716553\n",
            "Epoch 213, Loss: 0.6256405711174011\n",
            "Epoch 214, Loss: 0.6254870891571045\n",
            "Epoch 215, Loss: 0.6258434057235718\n",
            "Epoch 216, Loss: 0.6257899403572083\n",
            "Epoch 217, Loss: 0.6256844997406006\n",
            "Epoch 218, Loss: 0.6255432963371277\n",
            "Epoch 219, Loss: 0.6250187754631042\n",
            "Epoch 220, Loss: 0.625079870223999\n",
            "Epoch 221, Loss: 0.6252387166023254\n",
            "Epoch 222, Loss: 0.625444769859314\n",
            "Epoch 223, Loss: 0.6260385513305664\n",
            "Epoch 224, Loss: 0.6246337294578552\n",
            "Epoch 225, Loss: 0.6251299977302551\n",
            "Epoch 226, Loss: 0.625318706035614\n",
            "Epoch 227, Loss: 0.6253578662872314\n",
            "Epoch 228, Loss: 0.6246986985206604\n",
            "Epoch 229, Loss: 0.6248837113380432\n",
            "Epoch 230, Loss: 0.6244109869003296\n",
            "Epoch 231, Loss: 0.6245160698890686\n",
            "Epoch 232, Loss: 0.6246623992919922\n",
            "Epoch 233, Loss: 0.6253752112388611\n",
            "Epoch 234, Loss: 0.6240989565849304\n",
            "Epoch 235, Loss: 0.6249804496765137\n",
            "Epoch 236, Loss: 0.6241430640220642\n",
            "Epoch 237, Loss: 0.6253436803817749\n",
            "Epoch 238, Loss: 0.6240418553352356\n",
            "Epoch 239, Loss: 0.6242393851280212\n",
            "Epoch 240, Loss: 0.6244837045669556\n",
            "Epoch 241, Loss: 0.6239672899246216\n",
            "Epoch 242, Loss: 0.6241397261619568\n",
            "Epoch 243, Loss: 0.6246639490127563\n",
            "Epoch 244, Loss: 0.6246610879898071\n",
            "Epoch 245, Loss: 0.6240068674087524\n",
            "Epoch 246, Loss: 0.6243648529052734\n",
            "Epoch 247, Loss: 0.6238150596618652\n",
            "Epoch 248, Loss: 0.6237146854400635\n",
            "Epoch 249, Loss: 0.6237293481826782\n",
            "Epoch 250, Loss: 0.624011754989624\n",
            "Epoch 251, Loss: 0.6236865520477295\n",
            "Epoch 252, Loss: 0.6236680746078491\n",
            "Epoch 253, Loss: 0.6236605644226074\n",
            "Epoch 254, Loss: 0.623344361782074\n",
            "Epoch 255, Loss: 0.6232767701148987\n",
            "Epoch 256, Loss: 0.6232798099517822\n",
            "Epoch 257, Loss: 0.6232863664627075\n",
            "Epoch 258, Loss: 0.623142659664154\n",
            "Epoch 259, Loss: 0.6237013339996338\n",
            "Epoch 260, Loss: 0.6234083771705627\n",
            "Epoch 261, Loss: 0.6234660148620605\n",
            "Epoch 262, Loss: 0.6229349374771118\n",
            "Epoch 263, Loss: 0.6238446235656738\n",
            "Epoch 264, Loss: 0.6237394213676453\n",
            "Epoch 265, Loss: 0.6226541996002197\n",
            "Epoch 266, Loss: 0.6233227252960205\n",
            "Epoch 267, Loss: 0.6231229901313782\n",
            "Epoch 268, Loss: 0.6226687431335449\n",
            "Epoch 269, Loss: 0.6229821443557739\n",
            "Epoch 270, Loss: 0.6229265332221985\n",
            "Epoch 271, Loss: 0.6225661635398865\n",
            "Epoch 272, Loss: 0.6220806837081909\n",
            "Epoch 273, Loss: 0.6232813596725464\n",
            "Epoch 274, Loss: 0.6228170990943909\n",
            "Epoch 275, Loss: 0.62266606092453\n",
            "Epoch 276, Loss: 0.623009204864502\n",
            "Epoch 277, Loss: 0.6220621466636658\n",
            "Epoch 278, Loss: 0.6226545572280884\n",
            "Epoch 279, Loss: 0.6227059960365295\n",
            "Epoch 280, Loss: 0.6231656074523926\n",
            "Epoch 281, Loss: 0.6222673654556274\n",
            "Epoch 282, Loss: 0.6220760345458984\n",
            "Epoch 283, Loss: 0.6224029660224915\n",
            "Epoch 284, Loss: 0.6227123141288757\n",
            "Epoch 285, Loss: 0.6222254633903503\n",
            "Epoch 286, Loss: 0.622738242149353\n",
            "Epoch 287, Loss: 0.6222216486930847\n",
            "Epoch 288, Loss: 0.6225717067718506\n",
            "Epoch 289, Loss: 0.6219711899757385\n",
            "Epoch 290, Loss: 0.6226969957351685\n",
            "Epoch 291, Loss: 0.6217259168624878\n",
            "Epoch 292, Loss: 0.6222628951072693\n",
            "Epoch 293, Loss: 0.6226276755332947\n",
            "Epoch 294, Loss: 0.6224817633628845\n",
            "Epoch 295, Loss: 0.6220666170120239\n",
            "Epoch 296, Loss: 0.6206153035163879\n",
            "Epoch 297, Loss: 0.6216626167297363\n",
            "Epoch 298, Loss: 0.6222714185714722\n",
            "Epoch 299, Loss: 0.6219112873077393\n",
            "Epoch 300, Loss: 0.6216306090354919\n",
            "Epoch 301, Loss: 0.6217548251152039\n",
            "Epoch 302, Loss: 0.622101902961731\n",
            "Epoch 303, Loss: 0.6214984059333801\n",
            "Epoch 304, Loss: 0.6216604113578796\n",
            "Epoch 305, Loss: 0.6223796010017395\n",
            "Epoch 306, Loss: 0.6216604113578796\n",
            "Epoch 307, Loss: 0.6222078800201416\n",
            "Epoch 308, Loss: 0.6219736337661743\n",
            "Epoch 309, Loss: 0.6213091611862183\n",
            "Epoch 310, Loss: 0.6215648651123047\n",
            "Epoch 311, Loss: 0.6217053532600403\n",
            "Epoch 312, Loss: 0.6216686964035034\n",
            "Epoch 313, Loss: 0.6215162873268127\n",
            "Epoch 314, Loss: 0.6218755841255188\n",
            "Epoch 315, Loss: 0.6212410926818848\n",
            "Epoch 316, Loss: 0.621030330657959\n",
            "Epoch 317, Loss: 0.6212835907936096\n",
            "Epoch 318, Loss: 0.6220294833183289\n",
            "Epoch 319, Loss: 0.6215340495109558\n",
            "Epoch 320, Loss: 0.6218863129615784\n",
            "Epoch 321, Loss: 0.6217198371887207\n",
            "Epoch 322, Loss: 0.6210306882858276\n",
            "Epoch 323, Loss: 0.6216248869895935\n",
            "Epoch 324, Loss: 0.6216917634010315\n",
            "Epoch 325, Loss: 0.6209568381309509\n",
            "Epoch 326, Loss: 0.620673656463623\n",
            "Epoch 327, Loss: 0.6211974620819092\n",
            "Epoch 328, Loss: 0.6208617091178894\n",
            "Epoch 329, Loss: 0.6210897564888\n",
            "Epoch 330, Loss: 0.6203855872154236\n",
            "Epoch 331, Loss: 0.6206936240196228\n",
            "Epoch 332, Loss: 0.6207756996154785\n",
            "Epoch 333, Loss: 0.6213750839233398\n",
            "Epoch 334, Loss: 0.620866596698761\n",
            "Epoch 335, Loss: 0.6209102272987366\n",
            "Epoch 336, Loss: 0.621300220489502\n",
            "Epoch 337, Loss: 0.6208320260047913\n",
            "Epoch 338, Loss: 0.6211347579956055\n",
            "Epoch 339, Loss: 0.6212059855461121\n",
            "Epoch 340, Loss: 0.6207396984100342\n",
            "Epoch 341, Loss: 0.6207886338233948\n",
            "Epoch 342, Loss: 0.6207213997840881\n",
            "Epoch 343, Loss: 0.6208063364028931\n",
            "Epoch 344, Loss: 0.6205583214759827\n",
            "Epoch 345, Loss: 0.6211227774620056\n",
            "Epoch 346, Loss: 0.620876669883728\n",
            "Epoch 347, Loss: 0.6204032301902771\n",
            "Epoch 348, Loss: 0.6201373934745789\n",
            "Epoch 349, Loss: 0.6203222274780273\n",
            "Epoch 350, Loss: 0.6204252243041992\n",
            "Epoch 351, Loss: 0.6201216578483582\n",
            "Epoch 352, Loss: 0.6213430762290955\n",
            "Epoch 353, Loss: 0.62043696641922\n",
            "Epoch 354, Loss: 0.6206094026565552\n",
            "Epoch 355, Loss: 0.6206085681915283\n",
            "Epoch 356, Loss: 0.6195904016494751\n",
            "Epoch 357, Loss: 0.6209237575531006\n",
            "Epoch 358, Loss: 0.620589554309845\n",
            "Epoch 359, Loss: 0.6202049851417542\n",
            "Epoch 360, Loss: 0.6199169158935547\n",
            "Epoch 361, Loss: 0.620094358921051\n",
            "Epoch 362, Loss: 0.6202756762504578\n",
            "Epoch 363, Loss: 0.6203097105026245\n",
            "Epoch 364, Loss: 0.6196935176849365\n",
            "Epoch 365, Loss: 0.6205142140388489\n",
            "Epoch 366, Loss: 0.6198857426643372\n",
            "Epoch 367, Loss: 0.620651125907898\n",
            "Epoch 368, Loss: 0.6202220916748047\n",
            "Epoch 369, Loss: 0.620140790939331\n",
            "Epoch 370, Loss: 0.6205126643180847\n",
            "Epoch 371, Loss: 0.619975209236145\n",
            "Epoch 372, Loss: 0.6205292344093323\n",
            "Epoch 373, Loss: 0.6198070049285889\n",
            "Epoch 374, Loss: 0.6192910075187683\n",
            "Epoch 375, Loss: 0.620038628578186\n",
            "Epoch 376, Loss: 0.6199954152107239\n",
            "Epoch 377, Loss: 0.6193330883979797\n",
            "Epoch 378, Loss: 0.6199621558189392\n",
            "Epoch 379, Loss: 0.6198618412017822\n",
            "Epoch 380, Loss: 0.6199824213981628\n",
            "Epoch 381, Loss: 0.6202654242515564\n",
            "Epoch 382, Loss: 0.6204638481140137\n",
            "Epoch 383, Loss: 0.6192997694015503\n",
            "Epoch 384, Loss: 0.6197656393051147\n",
            "Epoch 385, Loss: 0.6202080845832825\n",
            "Epoch 386, Loss: 0.6199500560760498\n",
            "Epoch 387, Loss: 0.6197203397750854\n",
            "Epoch 388, Loss: 0.6197209358215332\n",
            "Epoch 389, Loss: 0.619899332523346\n",
            "Epoch 390, Loss: 0.6194334030151367\n",
            "Epoch 391, Loss: 0.6198548078536987\n",
            "Epoch 392, Loss: 0.6194285750389099\n",
            "Epoch 393, Loss: 0.6194731593132019\n",
            "Epoch 394, Loss: 0.6195188164710999\n",
            "Epoch 395, Loss: 0.6196521520614624\n",
            "Epoch 396, Loss: 0.6198477745056152\n",
            "Epoch 397, Loss: 0.6195039749145508\n",
            "Epoch 398, Loss: 0.6194430589675903\n",
            "Epoch 399, Loss: 0.6195193529129028\n",
            "Epoch 400, Loss: 0.6190192699432373\n",
            "Epoch 401, Loss: 0.6198267936706543\n",
            "Epoch 402, Loss: 0.6191399693489075\n",
            "Epoch 403, Loss: 0.6191845536231995\n",
            "Epoch 404, Loss: 0.6191498041152954\n",
            "Epoch 405, Loss: 0.620086669921875\n",
            "Epoch 406, Loss: 0.6195109486579895\n",
            "Epoch 407, Loss: 0.6193080544471741\n",
            "Epoch 408, Loss: 0.6192572712898254\n",
            "Epoch 409, Loss: 0.6191058158874512\n",
            "Epoch 410, Loss: 0.619340181350708\n",
            "Epoch 411, Loss: 0.6194978356361389\n",
            "Epoch 412, Loss: 0.6193035840988159\n",
            "Epoch 413, Loss: 0.618786096572876\n",
            "Epoch 414, Loss: 0.6192317008972168\n",
            "Epoch 415, Loss: 0.6193041205406189\n",
            "Epoch 416, Loss: 0.6194026470184326\n",
            "Epoch 417, Loss: 0.6190705895423889\n",
            "Epoch 418, Loss: 0.6193925142288208\n",
            "Epoch 419, Loss: 0.6192879676818848\n",
            "Epoch 420, Loss: 0.618908703327179\n",
            "Epoch 421, Loss: 0.6187813878059387\n",
            "Epoch 422, Loss: 0.619079053401947\n",
            "Epoch 423, Loss: 0.6186004877090454\n",
            "Epoch 424, Loss: 0.6190678477287292\n",
            "Epoch 425, Loss: 0.619067907333374\n",
            "Epoch 426, Loss: 0.6188144683837891\n",
            "Epoch 427, Loss: 0.6195288300514221\n",
            "Epoch 428, Loss: 0.6188526749610901\n",
            "Epoch 429, Loss: 0.6184253692626953\n",
            "Epoch 430, Loss: 0.6192041635513306\n",
            "Epoch 431, Loss: 0.6183072328567505\n",
            "Epoch 432, Loss: 0.6197305917739868\n",
            "Epoch 433, Loss: 0.6184564828872681\n",
            "Epoch 434, Loss: 0.6189208626747131\n",
            "Epoch 435, Loss: 0.6184020042419434\n",
            "Epoch 436, Loss: 0.6191742420196533\n",
            "Epoch 437, Loss: 0.6185773015022278\n",
            "Epoch 438, Loss: 0.6191712021827698\n",
            "Epoch 439, Loss: 0.6189537644386292\n",
            "Epoch 440, Loss: 0.6184329390525818\n",
            "Epoch 441, Loss: 0.6181212663650513\n",
            "Epoch 442, Loss: 0.6186484694480896\n",
            "Epoch 443, Loss: 0.6188188791275024\n",
            "Epoch 444, Loss: 0.6190347075462341\n",
            "Epoch 445, Loss: 0.6185745596885681\n",
            "Epoch 446, Loss: 0.6183858513832092\n",
            "Epoch 447, Loss: 0.6188967227935791\n",
            "Epoch 448, Loss: 0.6189773082733154\n",
            "Epoch 449, Loss: 0.6181372404098511\n",
            "Epoch 450, Loss: 0.6187320351600647\n",
            "Epoch 451, Loss: 0.617806077003479\n",
            "Epoch 452, Loss: 0.6183138489723206\n",
            "Epoch 453, Loss: 0.6183673739433289\n",
            "Epoch 454, Loss: 0.6183353662490845\n",
            "Epoch 455, Loss: 0.6184678077697754\n",
            "Epoch 456, Loss: 0.6186309456825256\n",
            "Epoch 457, Loss: 0.6185650825500488\n",
            "Epoch 458, Loss: 0.61844801902771\n",
            "Epoch 459, Loss: 0.6187138557434082\n",
            "Epoch 460, Loss: 0.618146538734436\n",
            "Epoch 461, Loss: 0.6177066564559937\n",
            "Epoch 462, Loss: 0.6184940338134766\n",
            "Epoch 463, Loss: 0.6182969808578491\n",
            "Epoch 464, Loss: 0.6179854869842529\n",
            "Epoch 465, Loss: 0.6182016730308533\n",
            "Epoch 466, Loss: 0.6185966730117798\n",
            "Epoch 467, Loss: 0.617892324924469\n",
            "Epoch 468, Loss: 0.6181639432907104\n",
            "Epoch 469, Loss: 0.6179537177085876\n",
            "Epoch 470, Loss: 0.618067741394043\n",
            "Epoch 471, Loss: 0.6176815032958984\n",
            "Epoch 472, Loss: 0.6181297898292542\n",
            "Epoch 473, Loss: 0.6186143755912781\n",
            "Epoch 474, Loss: 0.6179350018501282\n",
            "Epoch 475, Loss: 0.6174319386482239\n",
            "Epoch 476, Loss: 0.6175957918167114\n",
            "Epoch 477, Loss: 0.6177780628204346\n",
            "Epoch 478, Loss: 0.6184440851211548\n",
            "Epoch 479, Loss: 0.6184252500534058\n",
            "Epoch 480, Loss: 0.6181418895721436\n",
            "Epoch 481, Loss: 0.6183444857597351\n",
            "Epoch 482, Loss: 0.6177287101745605\n",
            "Epoch 483, Loss: 0.6184363961219788\n",
            "Epoch 484, Loss: 0.6177976727485657\n",
            "Epoch 485, Loss: 0.6180738210678101\n",
            "Epoch 486, Loss: 0.617345929145813\n",
            "Epoch 487, Loss: 0.6179221272468567\n",
            "Epoch 488, Loss: 0.6173763871192932\n",
            "Epoch 489, Loss: 0.6178616881370544\n",
            "Epoch 490, Loss: 0.6173895597457886\n",
            "Epoch 491, Loss: 0.6178101897239685\n",
            "Epoch 492, Loss: 0.6179037094116211\n",
            "Epoch 493, Loss: 0.6183192133903503\n",
            "Epoch 494, Loss: 0.6178869009017944\n",
            "Epoch 495, Loss: 0.6177000999450684\n",
            "Epoch 496, Loss: 0.6179182529449463\n",
            "Epoch 497, Loss: 0.6178947687149048\n",
            "Epoch 498, Loss: 0.618014395236969\n",
            "Epoch 499, Loss: 0.6177225708961487\n",
            "Epoch 500, Loss: 0.6174290180206299\n",
            "Epoch 501, Loss: 0.6177526712417603\n",
            "Epoch 502, Loss: 0.6177170276641846\n",
            "Epoch 503, Loss: 0.6172317266464233\n",
            "Epoch 504, Loss: 0.6172509789466858\n",
            "Epoch 505, Loss: 0.6177901029586792\n",
            "Epoch 506, Loss: 0.6174059510231018\n",
            "Epoch 507, Loss: 0.6175715327262878\n",
            "Epoch 508, Loss: 0.6180201172828674\n",
            "Epoch 509, Loss: 0.6176972389221191\n",
            "Epoch 510, Loss: 0.6172522306442261\n",
            "Epoch 511, Loss: 0.617506206035614\n",
            "Epoch 512, Loss: 0.6177428364753723\n",
            "Epoch 513, Loss: 0.617798388004303\n",
            "Epoch 514, Loss: 0.6177778840065002\n",
            "Epoch 515, Loss: 0.6179495453834534\n",
            "Epoch 516, Loss: 0.6174634695053101\n",
            "Epoch 517, Loss: 0.6179531216621399\n",
            "Epoch 518, Loss: 0.6171430945396423\n",
            "Epoch 519, Loss: 0.6173177361488342\n",
            "Epoch 520, Loss: 0.6178093552589417\n",
            "Epoch 521, Loss: 0.6176953315734863\n",
            "Epoch 522, Loss: 0.6176935434341431\n",
            "Epoch 523, Loss: 0.6184770464897156\n",
            "Epoch 524, Loss: 0.6171401739120483\n",
            "Epoch 525, Loss: 0.6170737743377686\n",
            "Epoch 526, Loss: 0.6176165342330933\n",
            "Epoch 527, Loss: 0.6172051429748535\n",
            "Epoch 528, Loss: 0.6175479888916016\n",
            "Epoch 529, Loss: 0.6176332831382751\n",
            "Epoch 530, Loss: 0.6175247430801392\n",
            "Epoch 531, Loss: 0.617711067199707\n",
            "Epoch 532, Loss: 0.6176398396492004\n",
            "Epoch 533, Loss: 0.6172987222671509\n",
            "Epoch 534, Loss: 0.6175529360771179\n",
            "Epoch 535, Loss: 0.6176007986068726\n",
            "Epoch 536, Loss: 0.6165894269943237\n",
            "Epoch 537, Loss: 0.6168814897537231\n",
            "Epoch 538, Loss: 0.6162193417549133\n",
            "Epoch 539, Loss: 0.6166709065437317\n",
            "Epoch 540, Loss: 0.6177075505256653\n",
            "Epoch 541, Loss: 0.6169077754020691\n",
            "Epoch 542, Loss: 0.6172014474868774\n",
            "Epoch 543, Loss: 0.6165605783462524\n",
            "Epoch 544, Loss: 0.6170077919960022\n",
            "Epoch 545, Loss: 0.6174145340919495\n",
            "Epoch 546, Loss: 0.6168915629386902\n",
            "Epoch 547, Loss: 0.6166518926620483\n",
            "Epoch 548, Loss: 0.6172206997871399\n",
            "Epoch 549, Loss: 0.6172934770584106\n",
            "Epoch 550, Loss: 0.6166802048683167\n",
            "Epoch 551, Loss: 0.6164686679840088\n",
            "Epoch 552, Loss: 0.6175554394721985\n",
            "Epoch 553, Loss: 0.6164078116416931\n",
            "Epoch 554, Loss: 0.6167964339256287\n",
            "Epoch 555, Loss: 0.6170881986618042\n",
            "Epoch 556, Loss: 0.6166654825210571\n",
            "Epoch 557, Loss: 0.6169175505638123\n",
            "Epoch 558, Loss: 0.6172559261322021\n",
            "Epoch 559, Loss: 0.6168155074119568\n",
            "Epoch 560, Loss: 0.6163573861122131\n",
            "Epoch 561, Loss: 0.6164755821228027\n",
            "Epoch 562, Loss: 0.6168627738952637\n",
            "Epoch 563, Loss: 0.6169235706329346\n",
            "Epoch 564, Loss: 0.6166012287139893\n",
            "Epoch 565, Loss: 0.6160783171653748\n",
            "Epoch 566, Loss: 0.6166630983352661\n",
            "Epoch 567, Loss: 0.61661696434021\n",
            "Epoch 568, Loss: 0.6170160174369812\n",
            "Epoch 569, Loss: 0.6170356273651123\n",
            "Epoch 570, Loss: 0.6164455413818359\n",
            "Epoch 571, Loss: 0.6164010167121887\n",
            "Epoch 572, Loss: 0.6163116693496704\n",
            "Epoch 573, Loss: 0.616804838180542\n",
            "Epoch 574, Loss: 0.6168116927146912\n",
            "Epoch 575, Loss: 0.6171696186065674\n",
            "Epoch 576, Loss: 0.617007851600647\n",
            "Epoch 577, Loss: 0.6166060566902161\n",
            "Epoch 578, Loss: 0.6179038286209106\n",
            "Epoch 579, Loss: 0.6167712211608887\n",
            "Epoch 580, Loss: 0.616701602935791\n",
            "Epoch 581, Loss: 0.6165809631347656\n",
            "Epoch 582, Loss: 0.6166671514511108\n",
            "Epoch 583, Loss: 0.6169028282165527\n",
            "Epoch 584, Loss: 0.6171598434448242\n",
            "Epoch 585, Loss: 0.6167619824409485\n",
            "Epoch 586, Loss: 0.6164812445640564\n",
            "Epoch 587, Loss: 0.616267740726471\n",
            "Epoch 588, Loss: 0.6164818406105042\n",
            "Epoch 589, Loss: 0.6166383624076843\n",
            "Epoch 590, Loss: 0.6167157888412476\n",
            "Epoch 591, Loss: 0.6160241961479187\n",
            "Epoch 592, Loss: 0.6165863871574402\n",
            "Epoch 593, Loss: 0.6159651875495911\n",
            "Epoch 594, Loss: 0.6164570450782776\n",
            "Epoch 595, Loss: 0.616585373878479\n",
            "Epoch 596, Loss: 0.6166975498199463\n",
            "Epoch 597, Loss: 0.6166907548904419\n",
            "Epoch 598, Loss: 0.6163565516471863\n",
            "Epoch 599, Loss: 0.6158402562141418\n",
            "Epoch 600, Loss: 0.6166084408760071\n",
            "Epoch 601, Loss: 0.6163370013237\n",
            "Epoch 602, Loss: 0.6158867478370667\n",
            "Epoch 603, Loss: 0.6165376305580139\n",
            "Epoch 604, Loss: 0.6154476404190063\n",
            "Epoch 605, Loss: 0.6160320043563843\n",
            "Epoch 606, Loss: 0.6165897250175476\n",
            "Epoch 607, Loss: 0.6162495613098145\n",
            "Epoch 608, Loss: 0.6165426969528198\n",
            "Epoch 609, Loss: 0.6162610650062561\n",
            "Epoch 610, Loss: 0.616129457950592\n",
            "Epoch 611, Loss: 0.6164454221725464\n",
            "Epoch 612, Loss: 0.6165321469306946\n",
            "Epoch 613, Loss: 0.6160224676132202\n",
            "Epoch 614, Loss: 0.6161112785339355\n",
            "Epoch 615, Loss: 0.6156297922134399\n",
            "Epoch 616, Loss: 0.6162508130073547\n",
            "Epoch 617, Loss: 0.6158158779144287\n",
            "Epoch 618, Loss: 0.6164857149124146\n",
            "Epoch 619, Loss: 0.6168866157531738\n",
            "Epoch 620, Loss: 0.6160533428192139\n",
            "Epoch 621, Loss: 0.616327702999115\n",
            "Epoch 622, Loss: 0.6158437728881836\n",
            "Epoch 623, Loss: 0.6161401271820068\n",
            "Epoch 624, Loss: 0.6163859367370605\n",
            "Epoch 625, Loss: 0.616206169128418\n",
            "Epoch 626, Loss: 0.6158891916275024\n",
            "Epoch 627, Loss: 0.6161651611328125\n",
            "Epoch 628, Loss: 0.614918053150177\n",
            "Epoch 629, Loss: 0.6159241199493408\n",
            "Epoch 630, Loss: 0.616348385810852\n",
            "Epoch 631, Loss: 0.6158725023269653\n",
            "Epoch 632, Loss: 0.6158249378204346\n",
            "Epoch 633, Loss: 0.6162914037704468\n",
            "Epoch 634, Loss: 0.6157466769218445\n",
            "Epoch 635, Loss: 0.6165770292282104\n",
            "Epoch 636, Loss: 0.6159402132034302\n",
            "Epoch 637, Loss: 0.6163300275802612\n",
            "Epoch 638, Loss: 0.6156793236732483\n",
            "Epoch 639, Loss: 0.6161134243011475\n",
            "Epoch 640, Loss: 0.615935742855072\n",
            "Epoch 641, Loss: 0.6157954931259155\n",
            "Epoch 642, Loss: 0.6163905262947083\n",
            "Epoch 643, Loss: 0.6154224872589111\n",
            "Epoch 644, Loss: 0.6152307987213135\n",
            "Epoch 645, Loss: 0.6161969900131226\n",
            "Epoch 646, Loss: 0.6165645718574524\n",
            "Epoch 647, Loss: 0.615942656993866\n",
            "Epoch 648, Loss: 0.6159657835960388\n",
            "Epoch 649, Loss: 0.6153086423873901\n",
            "Epoch 650, Loss: 0.6160374283790588\n",
            "Epoch 651, Loss: 0.6163121461868286\n",
            "Epoch 652, Loss: 0.6165480017662048\n",
            "Epoch 653, Loss: 0.6159297823905945\n",
            "Epoch 654, Loss: 0.6163311004638672\n",
            "Epoch 655, Loss: 0.616061270236969\n",
            "Epoch 656, Loss: 0.6162902116775513\n",
            "Epoch 657, Loss: 0.6160053610801697\n",
            "Epoch 658, Loss: 0.6166555881500244\n",
            "Epoch 659, Loss: 0.6153028011322021\n",
            "Epoch 660, Loss: 0.6159019470214844\n",
            "Epoch 661, Loss: 0.6154376864433289\n",
            "Epoch 662, Loss: 0.6159374117851257\n",
            "Epoch 663, Loss: 0.6156219244003296\n",
            "Epoch 664, Loss: 0.6162578463554382\n",
            "Epoch 665, Loss: 0.6157351732254028\n",
            "Epoch 666, Loss: 0.6159339547157288\n",
            "Epoch 667, Loss: 0.6166113615036011\n",
            "Epoch 668, Loss: 0.6157835125923157\n",
            "Epoch 669, Loss: 0.6157921552658081\n",
            "Epoch 670, Loss: 0.6156579852104187\n",
            "Epoch 671, Loss: 0.6161581873893738\n",
            "Epoch 672, Loss: 0.615761935710907\n",
            "Epoch 673, Loss: 0.6162064075469971\n",
            "Epoch 674, Loss: 0.615750253200531\n",
            "Epoch 675, Loss: 0.6157984137535095\n",
            "Epoch 676, Loss: 0.615330159664154\n",
            "Epoch 677, Loss: 0.6160626411437988\n",
            "Epoch 678, Loss: 0.6156635284423828\n",
            "Epoch 679, Loss: 0.6160284280776978\n",
            "Epoch 680, Loss: 0.6158618927001953\n",
            "Epoch 681, Loss: 0.6153962016105652\n",
            "Epoch 682, Loss: 0.6160088181495667\n",
            "Epoch 683, Loss: 0.6153351664543152\n",
            "Epoch 684, Loss: 0.6156597137451172\n",
            "Epoch 685, Loss: 0.614772379398346\n",
            "Epoch 686, Loss: 0.615918755531311\n",
            "Epoch 687, Loss: 0.6161152720451355\n",
            "Epoch 688, Loss: 0.6155784726142883\n",
            "Epoch 689, Loss: 0.615154504776001\n",
            "Epoch 690, Loss: 0.615731954574585\n",
            "Epoch 691, Loss: 0.6152173280715942\n",
            "Epoch 692, Loss: 0.6153606176376343\n",
            "Epoch 693, Loss: 0.6153458952903748\n",
            "Epoch 694, Loss: 0.6155690550804138\n",
            "Epoch 695, Loss: 0.6154858469963074\n",
            "Epoch 696, Loss: 0.6157227158546448\n",
            "Epoch 697, Loss: 0.6154763102531433\n",
            "Epoch 698, Loss: 0.6156563758850098\n",
            "Epoch 699, Loss: 0.6150258779525757\n",
            "Epoch 700, Loss: 0.6158723831176758\n",
            "Epoch 701, Loss: 0.6156494617462158\n",
            "Epoch 702, Loss: 0.6153519153594971\n",
            "Epoch 703, Loss: 0.6151067018508911\n",
            "Epoch 704, Loss: 0.6150318384170532\n",
            "Epoch 705, Loss: 0.615391194820404\n",
            "Epoch 706, Loss: 0.6156547665596008\n",
            "Epoch 707, Loss: 0.6155810356140137\n",
            "Epoch 708, Loss: 0.615034282207489\n",
            "Epoch 709, Loss: 0.6150972247123718\n",
            "Epoch 710, Loss: 0.614806592464447\n",
            "Epoch 711, Loss: 0.6159455180168152\n",
            "Epoch 712, Loss: 0.6155830025672913\n",
            "Epoch 713, Loss: 0.6150592565536499\n",
            "Epoch 714, Loss: 0.6157404184341431\n",
            "Epoch 715, Loss: 0.615386962890625\n",
            "Epoch 716, Loss: 0.6149266958236694\n",
            "Epoch 717, Loss: 0.6154727935791016\n",
            "Epoch 718, Loss: 0.6157795786857605\n",
            "Epoch 719, Loss: 0.6153108477592468\n",
            "Epoch 720, Loss: 0.6160297989845276\n",
            "Epoch 721, Loss: 0.6156044602394104\n",
            "Epoch 722, Loss: 0.6156248450279236\n",
            "Epoch 723, Loss: 0.615047037601471\n",
            "Epoch 724, Loss: 0.6152734756469727\n",
            "Epoch 725, Loss: 0.6154325604438782\n",
            "Epoch 726, Loss: 0.6154578328132629\n",
            "Epoch 727, Loss: 0.6149477958679199\n",
            "Epoch 728, Loss: 0.6153431534767151\n",
            "Epoch 729, Loss: 0.6148256063461304\n",
            "Epoch 730, Loss: 0.6152106523513794\n",
            "Epoch 731, Loss: 0.6140419840812683\n",
            "Epoch 732, Loss: 0.6153075695037842\n",
            "Epoch 733, Loss: 0.6147440075874329\n",
            "Epoch 734, Loss: 0.6147037744522095\n",
            "Epoch 735, Loss: 0.6148619651794434\n",
            "Epoch 736, Loss: 0.6153311729431152\n",
            "Epoch 737, Loss: 0.6148530840873718\n",
            "Epoch 738, Loss: 0.6155493259429932\n",
            "Epoch 739, Loss: 0.6147498488426208\n",
            "Epoch 740, Loss: 0.615041971206665\n",
            "Epoch 741, Loss: 0.614617645740509\n",
            "Epoch 742, Loss: 0.6151947975158691\n",
            "Epoch 743, Loss: 0.6147344708442688\n",
            "Epoch 744, Loss: 0.6150387525558472\n",
            "Epoch 745, Loss: 0.6146673560142517\n",
            "Epoch 746, Loss: 0.6150479316711426\n",
            "Epoch 747, Loss: 0.6142179369926453\n",
            "Epoch 748, Loss: 0.6146685481071472\n",
            "Epoch 749, Loss: 0.6147642135620117\n",
            "Epoch 750, Loss: 0.6148648262023926\n",
            "Epoch 751, Loss: 0.6150540113449097\n",
            "Epoch 752, Loss: 0.6143866777420044\n",
            "Epoch 753, Loss: 0.6150015592575073\n",
            "Epoch 754, Loss: 0.6143830418586731\n",
            "Epoch 755, Loss: 0.6148300170898438\n",
            "Epoch 756, Loss: 0.6146615147590637\n",
            "Epoch 757, Loss: 0.6146909594535828\n",
            "Epoch 758, Loss: 0.6142808198928833\n",
            "Epoch 759, Loss: 0.615694522857666\n",
            "Epoch 760, Loss: 0.6146160960197449\n",
            "Epoch 761, Loss: 0.6144192218780518\n",
            "Epoch 762, Loss: 0.6147754788398743\n",
            "Epoch 763, Loss: 0.6149493455886841\n",
            "Epoch 764, Loss: 0.613889217376709\n",
            "Epoch 765, Loss: 0.6147531270980835\n",
            "Epoch 766, Loss: 0.6156765818595886\n",
            "Epoch 767, Loss: 0.6148778200149536\n",
            "Epoch 768, Loss: 0.6151815056800842\n",
            "Epoch 769, Loss: 0.6143718361854553\n",
            "Epoch 770, Loss: 0.6153121590614319\n",
            "Epoch 771, Loss: 0.6146884560585022\n",
            "Epoch 772, Loss: 0.6153286099433899\n",
            "Epoch 773, Loss: 0.6143205761909485\n",
            "Epoch 774, Loss: 0.6146854162216187\n",
            "Epoch 775, Loss: 0.614555835723877\n",
            "Epoch 776, Loss: 0.6148891448974609\n",
            "Epoch 777, Loss: 0.6152569055557251\n",
            "Epoch 778, Loss: 0.6139824986457825\n",
            "Epoch 779, Loss: 0.6143571734428406\n",
            "Epoch 780, Loss: 0.6153222322463989\n",
            "Epoch 781, Loss: 0.6144911646842957\n",
            "Epoch 782, Loss: 0.6151818037033081\n",
            "Epoch 783, Loss: 0.6145121455192566\n",
            "Epoch 784, Loss: 0.6147322654724121\n",
            "Epoch 785, Loss: 0.6139698624610901\n",
            "Epoch 786, Loss: 0.6148194074630737\n",
            "Epoch 787, Loss: 0.614161491394043\n",
            "Epoch 788, Loss: 0.6151392459869385\n",
            "Epoch 789, Loss: 0.614353597164154\n",
            "Epoch 790, Loss: 0.6143476366996765\n",
            "Epoch 791, Loss: 0.6149284243583679\n",
            "Epoch 792, Loss: 0.6141785383224487\n",
            "Epoch 793, Loss: 0.6143208146095276\n",
            "Epoch 794, Loss: 0.6148718595504761\n",
            "Epoch 795, Loss: 0.6148311495780945\n",
            "Epoch 796, Loss: 0.6143953204154968\n",
            "Epoch 797, Loss: 0.6141146421432495\n",
            "Epoch 798, Loss: 0.6153302788734436\n",
            "Epoch 799, Loss: 0.6152998805046082\n",
            "Epoch 800, Loss: 0.6148824095726013\n",
            "Epoch 801, Loss: 0.6146153211593628\n",
            "Epoch 802, Loss: 0.6145224571228027\n",
            "Epoch 803, Loss: 0.6147117018699646\n",
            "Epoch 804, Loss: 0.6145658493041992\n",
            "Epoch 805, Loss: 0.6150341629981995\n",
            "Epoch 806, Loss: 0.6147032976150513\n",
            "Epoch 807, Loss: 0.6149000525474548\n",
            "Epoch 808, Loss: 0.6143028140068054\n",
            "Epoch 809, Loss: 0.6148093938827515\n",
            "Epoch 810, Loss: 0.6151983737945557\n",
            "Epoch 811, Loss: 0.6150104403495789\n",
            "Epoch 812, Loss: 0.6150850653648376\n",
            "Epoch 813, Loss: 0.6137617826461792\n",
            "Epoch 814, Loss: 0.6142951250076294\n",
            "Epoch 815, Loss: 0.6148210167884827\n",
            "Epoch 816, Loss: 0.6144124269485474\n",
            "Epoch 817, Loss: 0.613819420337677\n",
            "Epoch 818, Loss: 0.6145294308662415\n",
            "Epoch 819, Loss: 0.6145502328872681\n",
            "Epoch 820, Loss: 0.6140367388725281\n",
            "Epoch 821, Loss: 0.6141299605369568\n",
            "Epoch 822, Loss: 0.6145827770233154\n",
            "Epoch 823, Loss: 0.6144683957099915\n",
            "Epoch 824, Loss: 0.6144970059394836\n",
            "Epoch 825, Loss: 0.6140239238739014\n",
            "Epoch 826, Loss: 0.6142889857292175\n",
            "Epoch 827, Loss: 0.6138725280761719\n",
            "Epoch 828, Loss: 0.613990843296051\n",
            "Epoch 829, Loss: 0.6143717169761658\n",
            "Epoch 830, Loss: 0.6139597296714783\n",
            "Epoch 831, Loss: 0.6143596768379211\n",
            "Epoch 832, Loss: 0.6145188808441162\n",
            "Epoch 833, Loss: 0.6143825650215149\n",
            "Epoch 834, Loss: 0.6151638031005859\n",
            "Epoch 835, Loss: 0.6143967509269714\n",
            "Epoch 836, Loss: 0.6138599514961243\n",
            "Epoch 837, Loss: 0.6143945455551147\n",
            "Epoch 838, Loss: 0.6140331029891968\n",
            "Epoch 839, Loss: 0.6151229739189148\n",
            "Epoch 840, Loss: 0.6142823100090027\n",
            "Epoch 841, Loss: 0.6144853830337524\n",
            "Epoch 842, Loss: 0.6138780117034912\n",
            "Epoch 843, Loss: 0.6138564348220825\n",
            "Epoch 844, Loss: 0.6146144270896912\n",
            "Epoch 845, Loss: 0.6135977506637573\n",
            "Epoch 846, Loss: 0.6141698956489563\n",
            "Epoch 847, Loss: 0.6150118708610535\n",
            "Epoch 848, Loss: 0.6140549182891846\n",
            "Epoch 849, Loss: 0.6140735149383545\n",
            "Epoch 850, Loss: 0.614449143409729\n",
            "Epoch 851, Loss: 0.6139265894889832\n",
            "Epoch 852, Loss: 0.6141238808631897\n",
            "Epoch 853, Loss: 0.6146135330200195\n",
            "Epoch 854, Loss: 0.6139127612113953\n",
            "Epoch 855, Loss: 0.6144623756408691\n",
            "Epoch 856, Loss: 0.6150100827217102\n",
            "Epoch 857, Loss: 0.6141849160194397\n",
            "Epoch 858, Loss: 0.6140174269676208\n",
            "Epoch 859, Loss: 0.6141994595527649\n",
            "Epoch 860, Loss: 0.6145182847976685\n",
            "Epoch 861, Loss: 0.6141883730888367\n",
            "Epoch 862, Loss: 0.6139641404151917\n",
            "Epoch 863, Loss: 0.6134774684906006\n",
            "Epoch 864, Loss: 0.614346981048584\n",
            "Epoch 865, Loss: 0.614458441734314\n",
            "Epoch 866, Loss: 0.6142326593399048\n",
            "Epoch 867, Loss: 0.6143656969070435\n",
            "Epoch 868, Loss: 0.6137537956237793\n",
            "Epoch 869, Loss: 0.6143594980239868\n",
            "Epoch 870, Loss: 0.6137303113937378\n",
            "Epoch 871, Loss: 0.6142385601997375\n",
            "Epoch 872, Loss: 0.6146261096000671\n",
            "Epoch 873, Loss: 0.6142849326133728\n",
            "Epoch 874, Loss: 0.6142842769622803\n",
            "Epoch 875, Loss: 0.6142776012420654\n",
            "Epoch 876, Loss: 0.6139258742332458\n",
            "Epoch 877, Loss: 0.6136796474456787\n",
            "Epoch 878, Loss: 0.6141843199729919\n",
            "Epoch 879, Loss: 0.6140123009681702\n",
            "Epoch 880, Loss: 0.6143882870674133\n",
            "Epoch 881, Loss: 0.6138933300971985\n",
            "Epoch 882, Loss: 0.6142564415931702\n",
            "Epoch 883, Loss: 0.6135878562927246\n",
            "Epoch 884, Loss: 0.6139868497848511\n",
            "Epoch 885, Loss: 0.6139542460441589\n",
            "Epoch 886, Loss: 0.6145742535591125\n",
            "Epoch 887, Loss: 0.6146129965782166\n",
            "Epoch 888, Loss: 0.6138215661048889\n",
            "Epoch 889, Loss: 0.6138713955879211\n",
            "Epoch 890, Loss: 0.6140133142471313\n",
            "Epoch 891, Loss: 0.6148355007171631\n",
            "Epoch 892, Loss: 0.6144448518753052\n",
            "Epoch 893, Loss: 0.6141379475593567\n",
            "Epoch 894, Loss: 0.6134847402572632\n",
            "Epoch 895, Loss: 0.6144232153892517\n",
            "Epoch 896, Loss: 0.6138421297073364\n",
            "Epoch 897, Loss: 0.6137678623199463\n",
            "Epoch 898, Loss: 0.613915205001831\n",
            "Epoch 899, Loss: 0.6142271161079407\n",
            "Epoch 900, Loss: 0.6135556101799011\n",
            "Epoch 901, Loss: 0.6143306493759155\n",
            "Epoch 902, Loss: 0.6141026020050049\n",
            "Epoch 903, Loss: 0.6137770414352417\n",
            "Epoch 904, Loss: 0.6137819886207581\n",
            "Epoch 905, Loss: 0.6135059595108032\n",
            "Epoch 906, Loss: 0.6137095093727112\n",
            "Epoch 907, Loss: 0.6136187314987183\n",
            "Epoch 908, Loss: 0.613574743270874\n",
            "Epoch 909, Loss: 0.6136044859886169\n",
            "Epoch 910, Loss: 0.6133883595466614\n",
            "Epoch 911, Loss: 0.6134677529335022\n",
            "Epoch 912, Loss: 0.6137669682502747\n",
            "Epoch 913, Loss: 0.6144989132881165\n",
            "Epoch 914, Loss: 0.613875687122345\n",
            "Epoch 915, Loss: 0.6138516068458557\n",
            "Epoch 916, Loss: 0.6136782765388489\n",
            "Epoch 917, Loss: 0.6146414279937744\n",
            "Epoch 918, Loss: 0.6134431958198547\n",
            "Epoch 919, Loss: 0.6138122081756592\n",
            "Epoch 920, Loss: 0.6138386726379395\n",
            "Epoch 921, Loss: 0.6138753890991211\n",
            "Epoch 922, Loss: 0.6137768030166626\n",
            "Epoch 923, Loss: 0.6139397025108337\n",
            "Epoch 924, Loss: 0.6141325831413269\n",
            "Epoch 925, Loss: 0.6135501265525818\n",
            "Epoch 926, Loss: 0.6136637330055237\n",
            "Epoch 927, Loss: 0.6139933466911316\n",
            "Epoch 928, Loss: 0.6141753792762756\n",
            "Epoch 929, Loss: 0.613706648349762\n",
            "Epoch 930, Loss: 0.6142876148223877\n",
            "Epoch 931, Loss: 0.613045334815979\n",
            "Epoch 932, Loss: 0.6142361760139465\n",
            "Epoch 933, Loss: 0.61379075050354\n",
            "Epoch 934, Loss: 0.6131749153137207\n",
            "Epoch 935, Loss: 0.6133696436882019\n",
            "Epoch 936, Loss: 0.614007830619812\n",
            "Epoch 937, Loss: 0.6139461994171143\n",
            "Epoch 938, Loss: 0.6143015623092651\n",
            "Epoch 939, Loss: 0.613422691822052\n",
            "Epoch 940, Loss: 0.613718569278717\n",
            "Epoch 941, Loss: 0.6133912801742554\n",
            "Epoch 942, Loss: 0.6136863827705383\n",
            "Epoch 943, Loss: 0.6139417290687561\n",
            "Epoch 944, Loss: 0.6132736802101135\n",
            "Epoch 945, Loss: 0.6139708757400513\n",
            "Epoch 946, Loss: 0.6135663390159607\n",
            "Epoch 947, Loss: 0.6133695840835571\n",
            "Epoch 948, Loss: 0.6132799386978149\n",
            "Epoch 949, Loss: 0.6136316061019897\n",
            "Epoch 950, Loss: 0.6137015223503113\n",
            "Epoch 951, Loss: 0.6139017343521118\n",
            "Epoch 952, Loss: 0.6140175461769104\n",
            "Epoch 953, Loss: 0.6139927506446838\n",
            "Epoch 954, Loss: 0.6138930320739746\n",
            "Epoch 955, Loss: 0.6133431792259216\n",
            "Epoch 956, Loss: 0.6131739020347595\n",
            "Epoch 957, Loss: 0.6138839721679688\n",
            "Epoch 958, Loss: 0.6137831807136536\n",
            "Epoch 959, Loss: 0.6138216257095337\n",
            "Epoch 960, Loss: 0.6134931445121765\n",
            "Epoch 961, Loss: 0.6134727001190186\n",
            "Epoch 962, Loss: 0.6135858297348022\n",
            "Epoch 963, Loss: 0.6138474345207214\n",
            "Epoch 964, Loss: 0.6140902042388916\n",
            "Epoch 965, Loss: 0.6131112575531006\n",
            "Epoch 966, Loss: 0.6135692000389099\n",
            "Epoch 967, Loss: 0.6136981248855591\n",
            "Epoch 968, Loss: 0.6133227348327637\n",
            "Epoch 969, Loss: 0.6137462258338928\n",
            "Epoch 970, Loss: 0.6134034991264343\n",
            "Epoch 971, Loss: 0.6131882071495056\n",
            "Epoch 972, Loss: 0.6129938960075378\n",
            "Epoch 973, Loss: 0.6134835481643677\n",
            "Epoch 974, Loss: 0.613280177116394\n",
            "Epoch 975, Loss: 0.6130830645561218\n",
            "Epoch 976, Loss: 0.6134884357452393\n",
            "Epoch 977, Loss: 0.613814651966095\n",
            "Epoch 978, Loss: 0.6133469939231873\n",
            "Epoch 979, Loss: 0.6139687299728394\n",
            "Epoch 980, Loss: 0.613752543926239\n",
            "Epoch 981, Loss: 0.6130247116088867\n",
            "Epoch 982, Loss: 0.6132517457008362\n",
            "Epoch 983, Loss: 0.6137956976890564\n",
            "Epoch 984, Loss: 0.6138715147972107\n",
            "Epoch 985, Loss: 0.6127070784568787\n",
            "Epoch 986, Loss: 0.6140069961547852\n",
            "Epoch 987, Loss: 0.6134974360466003\n",
            "Epoch 988, Loss: 0.6131994724273682\n",
            "Epoch 989, Loss: 0.6130698323249817\n",
            "Epoch 990, Loss: 0.613349437713623\n",
            "Epoch 991, Loss: 0.6131150722503662\n",
            "Epoch 992, Loss: 0.6128643155097961\n",
            "Epoch 993, Loss: 0.6131502985954285\n",
            "Epoch 994, Loss: 0.6127960085868835\n",
            "Epoch 995, Loss: 0.6131298542022705\n",
            "Epoch 996, Loss: 0.6131983399391174\n",
            "Epoch 997, Loss: 0.6137039065361023\n",
            "Epoch 998, Loss: 0.613342821598053\n",
            "Epoch 999, Loss: 0.6127906441688538\n",
            "Epoch 1000, Loss: 0.613235354423523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "AMSLn0PIleGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import jellyfish\n",
        "\n",
        "# Prepare validation data for the first 500 entries\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    if len(val_data) >= 500:\n",
        "        break\n",
        "    ing1 = row['ingredient1']\n",
        "    combined_embedding = get_combined_embedding(ing1, model.wv, graph_embeddings)\n",
        "\n",
        "    val_data.append(combined_embedding)\n",
        "    val_labels.append(row['ingredient2'])\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
        "val_predictions = nn_model(val_data).detach().numpy()\n",
        "\n",
        "# Function to find the top N most similar ingredients based on cosine similarity\n",
        "def find_top_similar_ingredients(predicted_embedding, combined_embeddings, top_n=10):\n",
        "    similarities = {}\n",
        "    for ingredient, embedding in combined_embeddings.items():\n",
        "        similarity = cosine_similarity(predicted_embedding.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
        "        similarities[ingredient] = similarity\n",
        "    sorted_ingredients = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [ingredient for ingredient, similarity in sorted_ingredients[:top_n]]\n",
        "\n",
        "# Function to calculate metrics with Jaro-Winkler similarity threshold\n",
        "def calculate_metrics(predictions, ground_truths, combined_embeddings, top_n=10, threshold=0.8):\n",
        "    mrr, hit_1, hit_3, hit_10 = 0.0, 0.0, 0.0, 0.0\n",
        "    total = len(ground_truths)\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        top_similar = find_top_similar_ingredients(pred, combined_embeddings, top_n=top_n)\n",
        "        for rank, candidate in enumerate(top_similar, start=1):\n",
        "            sim = jellyfish.jaro_winkler_similarity(gt, candidate)\n",
        "            if sim >= threshold:\n",
        "                mrr += 1.0 / rank\n",
        "                if rank == 1:\n",
        "                    hit_1 += 1.0\n",
        "                if rank <= 3:\n",
        "                    hit_3 += 1.0\n",
        "                if rank <= 10:\n",
        "                    hit_10 += 1.0\n",
        "                break\n",
        "\n",
        "    mrr /= total\n",
        "    hit_1 /= total\n",
        "    hit_3 /= total\n",
        "    hit_10 /= total\n",
        "    return mrr, hit_1, hit_3, hit_10\n",
        "\n",
        "# Calculate metrics for the first 500 entries of the validation set\n",
        "val_labels_str = val_labels  # Assuming labels are ingredient names\n",
        "combined_embeddings = {ingredient: get_combined_embedding(ingredient, model.wv, graph_embeddings) for ingredient in model.wv.index_to_key}\n",
        "mrr, hit_1, hit_3, hit_10 = calculate_metrics(val_predictions, val_labels_str, combined_embeddings)\n",
        "\n",
        "print(f\"MRR: {mrr:.4f}, Hit@1: {hit_1:.4f}, Hit@3: {hit_3:.4f}, Hit@10: {hit_10:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HAv3RHXvaGw",
        "outputId": "af776c19-b22d-4f63-e8a1-760d73aa393f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR: 0.1622, Hit@1: 0.1020, Hit@3: 0.1920, Hit@10: 0.3160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Save the graph embeddings\n",
        "with open('graph_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(graph_embeddings, f)\n",
        "\n",
        "# Save the Node2Vec model\n",
        "graph_model.save('/content/drive/My Drive/ERP/node2vec_model_actual.model')"
      ],
      "metadata": {
        "id": "ccyXFli-Ibkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the graph embeddings\n",
        "with open('graph_embeddings.pkl', 'rb') as f:\n",
        "    graph_embeddings = pickle.load(f)\n",
        "\n",
        "# Load the Node2Vec model\n",
        "graph_model = Word2Vec.load('/content/drive/My Drive/ERP/node2vec_model_actual.model')\n",
        "\n",
        "# Now you can use the loaded graph embeddings and model as needed"
      ],
      "metadata": {
        "id": "koG5dbCIIgIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}