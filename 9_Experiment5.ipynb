{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "20Vci0VaPvBx",
        "V3bXs1g3P4c8"
      ],
      "authorship_tag": "ABX9TyPV+JJB5tbU7RPgAFQZTk15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afca12eaece04c568061a8e8ccb35ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f985fb356794a0eba5befe1f7da93ab",
              "IPY_MODEL_b44cdb8e30f44e0abb447c1993ab00dd",
              "IPY_MODEL_2b7bdcace28b4e85b477870e5df3bd35"
            ],
            "layout": "IPY_MODEL_ec8a82f2287349fc8113ee43cd21ad55"
          }
        },
        "2f985fb356794a0eba5befe1f7da93ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a54b86505fbe439787aee2d1196421d2",
            "placeholder": "​",
            "style": "IPY_MODEL_c095628903254a91ad7df23cce118439",
            "value": "Computing transition probabilities: 100%"
          }
        },
        "b44cdb8e30f44e0abb447c1993ab00dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b184606a88454dd0b77f5cdfc6d861fe",
            "max": 6651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4d1a811a7f34b208c79406499d35044",
            "value": 6651
          }
        },
        "2b7bdcace28b4e85b477870e5df3bd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd067e579e0447b9f15c106d384fe9c",
            "placeholder": "​",
            "style": "IPY_MODEL_20ff68b1de834b58971e47a8c738dd75",
            "value": " 6651/6651 [04:48&lt;00:00, 19.86it/s]"
          }
        },
        "ec8a82f2287349fc8113ee43cd21ad55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a54b86505fbe439787aee2d1196421d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c095628903254a91ad7df23cce118439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b184606a88454dd0b77f5cdfc6d861fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d1a811a7f34b208c79406499d35044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fd067e579e0447b9f15c106d384fe9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ff68b1de834b58971e47a8c738dd75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JITHIN-ANTONY-JOSEPH/ERP_11358080/blob/main/9_Experiment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input : Recipe Instructions\n",
        "### Model : Word2Vec(Text) + Node2Vec(Graph) + Attention Based Embeddings Neural Networks(Model)"
      ],
      "metadata": {
        "id": "C77aCBNzaxVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to connect to Google Drive"
      ],
      "metadata": {
        "id": "20Vci0VaPvBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3RMueg-Fgbo",
        "outputId": "316eb347-7a3f-4ffc-dcbd-51ff6092b61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing required libraries"
      ],
      "metadata": {
        "id": "V3bXs1g3P4c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNoHd2KrISPx",
        "outputId": "94af2511-dba6-4fc4-d2a7-6a884407cd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.66.5)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.16.0)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: node2vec\n",
            "Successfully installed node2vec-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries , loading datasets and pre-processing"
      ],
      "metadata": {
        "id": "GR3B29wXPyJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "V0KfNME9Fxjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity  # Add this import"
      ],
      "metadata": {
        "id": "VBBoL1hjWy-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flavorgraph_df = pd.read_csv('/content/drive/My Drive/ERP/Dataset/nodes_191120.csv')  # Adjust the path as needed , this is the path to my personal Google Drive"
      ],
      "metadata": {
        "id": "PBNmm2oYF6m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import jellyfish\n",
        "from gensim.models import Word2Vec\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Load the main dataset\n",
        "with open('/content/drive/My Drive/ERP/modified_Processed_Layer1.json', 'r') as file:# Adjust the path as needed , this is the path to my personal Google Drive\n",
        "    recipe1m_data = [json.loads(line) for line in file]\n",
        "\n",
        "recipe1m_df = pd.DataFrame(recipe1m_data)\n",
        "\n",
        "# Load the substitution pairs\n",
        "substitution_pairs_df = pd.read_csv('/content/drive/My Drive/ERP/Recipe1MSubs_full.csv')# Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Merge the datasets based on recipe_id (substitution_pairs_df) and id (recipe1m_df)\n",
        "merged_df = pd.merge(recipe1m_df, substitution_pairs_df, left_on='id', right_on='recipe_id')\n",
        "\n",
        "# Example ingredient list for NER-like extraction (replace with your own comprehensive list or use NER model)\n",
        "ingredient_list = set(flavorgraph_df[flavorgraph_df['node_type'] == 'ingredient']['name'].dropna().unique())\n",
        "\n",
        "# Function to extract ingredients from instructions\n",
        "def extract_ingredients_from_instructions(instructions, ingredient_list):\n",
        "    extracted_ingredients = []\n",
        "    for instruction in instructions:\n",
        "        words = instruction.split()\n",
        "        for word in words:\n",
        "            if word in ingredient_list:\n",
        "                extracted_ingredients.append(word)\n",
        "    return extracted_ingredients\n",
        "\n",
        "# Apply the extraction function\n",
        "recipe1m_df['extracted_ingredients'] = recipe1m_df['processed_instructions'].apply(\n",
        "    lambda instructions: extract_ingredients_from_instructions(instructions, ingredient_list) if isinstance(instructions, list) else []\n",
        ")\n",
        "\n",
        "# Prepare sentences for training\n",
        "sentences = recipe1m_df['extracted_ingredients'].tolist()\n",
        "\n",
        "# Add substitution contexts to sentences\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ingredient1 = row['ingredient1']\n",
        "    ingredient2 = row['ingredient2']\n",
        "    sentences.append([ingredient1, ingredient2])\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=8)  # Increase 'workers' to utilize more CPU cores"
      ],
      "metadata": {
        "id": "FEqKlaW7GCap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating graph embeddings"
      ],
      "metadata": {
        "id": "mLvY432_P-Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "# Load the knowledge graph\n",
        "flavor_graph = nx.read_graphml('/content/drive/My Drive/ERP/knowledge_graph.graphml') # Adjust the path as needed , this is the path to my personal Google Drive\n",
        "\n",
        "# Filter the graph to include only ingredient nodes\n",
        "ingredient_nodes = [n for n, attr in flavor_graph.nodes(data=True) if attr['node_type'] == 'ingredient']\n",
        "flavor_graph = flavor_graph.subgraph(ingredient_nodes)\n",
        "\n",
        "# Generate Node2Vec embeddings considering edge weights\n",
        "node2vec = Node2Vec(flavor_graph, dimensions=100, walk_length=30, num_walks=200, workers=4, weight_key='weight')\n",
        "graph_model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "# Generate graph embeddings for the ingredients\n",
        "graph_embeddings = {str(node): graph_model.wv[str(node)] for node in flavor_graph.nodes()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "afca12eaece04c568061a8e8ccb35ea7",
            "2f985fb356794a0eba5befe1f7da93ab",
            "b44cdb8e30f44e0abb447c1993ab00dd",
            "2b7bdcace28b4e85b477870e5df3bd35",
            "ec8a82f2287349fc8113ee43cd21ad55",
            "a54b86505fbe439787aee2d1196421d2",
            "c095628903254a91ad7df23cce118439",
            "b184606a88454dd0b77f5cdfc6d861fe",
            "c4d1a811a7f34b208c79406499d35044",
            "5fd067e579e0447b9f15c106d384fe9c",
            "20ff68b1de834b58971e47a8c738dd75"
          ]
        },
        "id": "j6h3SbwjHn-K",
        "outputId": "a94d0878-4c31-4612-8288-7b55c823ef23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/6651 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afca12eaece04c568061a8e8ccb35ea7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention based combined embeddings and defining the combined model"
      ],
      "metadata": {
        "id": "v3BSZ_-yQCgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import jellyfish\n",
        "\n",
        "# Assuming text_embeddings contains Word2Vec embeddings and graph_embeddings contains Node2Vec embeddings\n",
        "text_embeddings = model.wv  # Word2Vec model\n",
        "graph_embeddings = {str(node): graph_model.wv[str(node)] for node in flavor_graph.nodes()}  # Node2Vec model\n",
        "\n",
        "# Define a neural network with attention mechanisms to combine text and graph embeddings\n",
        "class CombinedNNWithAttention(nn.Module):\n",
        "    def __init__(self, text_dim, graph_dim, combined_dim):\n",
        "        super(CombinedNNWithAttention, self).__init__()\n",
        "        self.text_fc = nn.Linear(text_dim, combined_dim)\n",
        "        self.graph_fc = nn.Linear(graph_dim, combined_dim)\n",
        "        self.attention = nn.Parameter(torch.randn(2, 1))  # Adjust shape to (2, 1)\n",
        "\n",
        "    def forward(self, text_embedding, graph_embedding):\n",
        "        text_proj = self.text_fc(text_embedding)\n",
        "        graph_proj = self.graph_fc(graph_embedding)\n",
        "\n",
        "        combined = torch.stack([text_proj, graph_proj], dim=1)  # Shape: (batch_size, 2, combined_dim)\n",
        "        attention_weights = F.softmax(torch.matmul(combined, self.attention), dim=1)  # Shape: (batch_size, 2, 1)\n",
        "\n",
        "        weighted_sum = (combined * attention_weights).sum(dim=1)  # Shape: (batch_size, combined_dim)\n",
        "        return weighted_sum\n",
        "\n",
        "# Instantiate the attention-based model\n",
        "combined_model = CombinedNNWithAttention(text_dim=100, graph_dim=100, combined_dim=128)\n",
        "\n",
        "# Example of getting combined embeddings using the attention-based model\n",
        "def get_combined_embedding_with_attention(ingredient, text_embeddings, graph_embeddings, model):\n",
        "    text_embedding = torch.tensor(text_embeddings[ingredient] if ingredient in text_embeddings else np.zeros(100), dtype=torch.float32)\n",
        "    graph_embedding = torch.tensor(graph_embeddings[ingredient] if ingredient in graph_embeddings else np.zeros(100), dtype=torch.float32)\n",
        "    combined_embedding = model(text_embedding, graph_embedding)\n",
        "    return combined_embedding\n",
        "\n",
        "# Combine embeddings for training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for _, row in substitution_pairs_df.iterrows():\n",
        "    ing1 = row['ingredient1']\n",
        "    ing2 = row['ingredient2']\n",
        "    combined_embedding1 = get_combined_embedding_with_attention(ing1, text_embeddings, graph_embeddings, combined_model)\n",
        "    combined_embedding2 = get_combined_embedding_with_attention(ing2, text_embeddings, graph_embeddings, combined_model)\n",
        "\n",
        "    train_data.append(combined_embedding1.detach().numpy())\n",
        "    train_labels.append(combined_embedding2.detach().numpy())\n",
        "\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network for training\n",
        "class CombinedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(CombinedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate and train the improved model\n",
        "improved_nn_model = CombinedNN(input_dim=128, hidden_dim=256, output_dim=128)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(improved_nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # Adjust epochs as needed\n",
        "    improved_nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = improved_nn_model(train_data)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for idx, row in enumerate(substitution_pairs_df.itertuples()):\n",
        "    if idx >= 500:\n",
        "        break\n",
        "    ing1 = row.ingredient1\n",
        "    combined_embedding = get_combined_embedding_with_attention(ing1, text_embeddings, graph_embeddings, combined_model)\n",
        "\n",
        "    val_data.append(combined_embedding.detach().numpy())\n",
        "    val_labels.append(row.ingredient2)\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
        "val_predictions = improved_nn_model(val_data).detach().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "WSCBl4-1IlNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e694fd-69d9-461d-c7d5-ca1a30c2560b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1b1669861525>:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  train_data = torch.tensor(train_data, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.1614018678665161\n",
            "Epoch 2, Loss: 1.0876461267471313\n",
            "Epoch 3, Loss: 1.0271925926208496\n",
            "Epoch 4, Loss: 0.9745786786079407\n",
            "Epoch 5, Loss: 0.9298862814903259\n",
            "Epoch 6, Loss: 0.8939566016197205\n",
            "Epoch 7, Loss: 0.8621975183486938\n",
            "Epoch 8, Loss: 0.8344807028770447\n",
            "Epoch 9, Loss: 0.8104305267333984\n",
            "Epoch 10, Loss: 0.7878848910331726\n",
            "Epoch 11, Loss: 0.7688062191009521\n",
            "Epoch 12, Loss: 0.7495964169502258\n",
            "Epoch 13, Loss: 0.7323254942893982\n",
            "Epoch 14, Loss: 0.7157021164894104\n",
            "Epoch 15, Loss: 0.7008469700813293\n",
            "Epoch 16, Loss: 0.6874053478240967\n",
            "Epoch 17, Loss: 0.6741859912872314\n",
            "Epoch 18, Loss: 0.6612924337387085\n",
            "Epoch 19, Loss: 0.649154543876648\n",
            "Epoch 20, Loss: 0.6395578980445862\n",
            "Epoch 21, Loss: 0.6286355257034302\n",
            "Epoch 22, Loss: 0.6188615560531616\n",
            "Epoch 23, Loss: 0.6088233590126038\n",
            "Epoch 24, Loss: 0.6010745763778687\n",
            "Epoch 25, Loss: 0.5926738977432251\n",
            "Epoch 26, Loss: 0.5845348238945007\n",
            "Epoch 27, Loss: 0.5770642161369324\n",
            "Epoch 28, Loss: 0.5698931813240051\n",
            "Epoch 29, Loss: 0.5630342960357666\n",
            "Epoch 30, Loss: 0.556881308555603\n",
            "Epoch 31, Loss: 0.5500476956367493\n",
            "Epoch 32, Loss: 0.5444405674934387\n",
            "Epoch 33, Loss: 0.538856029510498\n",
            "Epoch 34, Loss: 0.5335176587104797\n",
            "Epoch 35, Loss: 0.5287109017372131\n",
            "Epoch 36, Loss: 0.5236042141914368\n",
            "Epoch 37, Loss: 0.5192503333091736\n",
            "Epoch 38, Loss: 0.5150172710418701\n",
            "Epoch 39, Loss: 0.5107330679893494\n",
            "Epoch 40, Loss: 0.5065729022026062\n",
            "Epoch 41, Loss: 0.503129243850708\n",
            "Epoch 42, Loss: 0.4991346597671509\n",
            "Epoch 43, Loss: 0.4958181083202362\n",
            "Epoch 44, Loss: 0.492713987827301\n",
            "Epoch 45, Loss: 0.48916691541671753\n",
            "Epoch 46, Loss: 0.48651960492134094\n",
            "Epoch 47, Loss: 0.48399457335472107\n",
            "Epoch 48, Loss: 0.4807772636413574\n",
            "Epoch 49, Loss: 0.47855108976364136\n",
            "Epoch 50, Loss: 0.47582393884658813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gtUdnThUQJ_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation data for the first 100 entries\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for idx, row in enumerate(substitution_pairs_df.itertuples()):\n",
        "    if idx >= 500:\n",
        "        break\n",
        "    ing1 = row.ingredient1\n",
        "    combined_embedding = get_combined_embedding_with_attention(ing1, text_embeddings, graph_embeddings, combined_model)\n",
        "\n",
        "    val_data.append(combined_embedding.detach().numpy())\n",
        "    val_labels.append(row.ingredient2)\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
        "\n",
        "# Generate predictions\n",
        "improved_nn_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    val_predictions = improved_nn_model(val_data).detach().numpy()\n",
        "\n",
        "# Function to find the top N most similar ingredients based on cosine similarity\n",
        "def find_top_similar_ingredients(predicted_embedding, combined_embeddings, top_n=10):\n",
        "    similarities = {}\n",
        "    for ingredient, embedding in combined_embeddings.items():\n",
        "        similarity = cosine_similarity(predicted_embedding.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
        "        similarities[ingredient] = similarity\n",
        "    sorted_ingredients = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [ingredient for ingredient, similarity in sorted_ingredients[:top_n]]\n",
        "\n",
        "# Function to calculate metrics with Jaro-Winkler similarity threshold\n",
        "def calculate_metrics(predictions, ground_truths, combined_embeddings, top_n=10, threshold=0.8):\n",
        "    mrr, hit_1, hit_3, hit_10 = 0.0, 0.0, 0.0, 0.0\n",
        "    total = len(ground_truths)\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        top_similar = find_top_similar_ingredients(pred, combined_embeddings, top_n=top_n)\n",
        "        for rank, candidate in enumerate(top_similar, start=1):\n",
        "            sim = jellyfish.jaro_winkler_similarity(gt, candidate)\n",
        "            if sim >= threshold:\n",
        "                mrr += 1.0 / rank\n",
        "                if rank == 1:\n",
        "                    hit_1 += 1.0\n",
        "                if rank <= 3:\n",
        "                    hit_3 += 1.0\n",
        "                if rank <= 10:\n",
        "                    hit_10 += 1.0\n",
        "                break\n",
        "\n",
        "    mrr /= total\n",
        "    hit_1 /= total\n",
        "    hit_3 /= total\n",
        "    hit_10 /= total\n",
        "    return mrr, hit_1, hit_3, hit_10\n",
        "\n",
        "# Calculate metrics for the first 100 entries of the validation set\n",
        "val_labels_str = val_labels  # Assuming labels are ingredient names\n",
        "combined_embeddings = {ingredient: get_combined_embedding_with_attention(ingredient, text_embeddings, graph_embeddings, combined_model).detach().numpy() for ingredient in text_embeddings.index_to_key}\n",
        "mrr, hit_1, hit_3, hit_10 = calculate_metrics(val_predictions, val_labels_str, combined_embeddings)\n",
        "\n",
        "print(f\"MRR: {mrr:.4f}, Hit@1: {hit_1:.4f}, Hit@3: {hit_3:.4f}, Hit@10: {hit_10:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ6qprNHZRYk",
        "outputId": "4ad3d9ed-324c-4370-98ce-bd1e09a7f700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR: 0.1071, Hit@1: 0.0560, Hit@3: 0.1300, Hit@10: 0.2440\n"
          ]
        }
      ]
    }
  ]
}